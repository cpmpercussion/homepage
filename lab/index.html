<!DOCTYPE html>
<html lang="en-AU">
  <head>
	  <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>SMCCLAB: Sound, Music and Creative Computing Lab</title>
<meta name="description" content="">
<!-- favicons --> 
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#e0e0e0">
<!-- styles --> 
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<link rel="stylesheet" href="/assets/css/academicons.min.css" />
<link rel="stylesheet" type="text/css" href='/assets/css/main.css'>
<!-- Bootstrap CSS -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-aFq/bzH65dt+w6FI2ooMVUpc+21e0SRygnTpmBvdBgSdnuTN7QbdgL+OapgHtvPp" crossorigin="anonymous">

  </head>
  <body>
    <nav class="navbar navbar-expand-lg bg-dark" data-bs-theme="dark">
  <div class="container-fluid">
  <a class="navbar-brand" href="#">
    <img src="/assets/images/cpm-logo-white.svg" width="40" class="d-inline-block align-top" alt="Logo">
    cpm
  </a>
  <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
  <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
    <ul class="navbar-nav">
      
      
      
      <li class="nav-item ">
      <a class="nav-link " href="/">home</a>
      </li>
      
      
      
      <li class="nav-item ">
      <a class="nav-link " href="/blog/">blog</a>
      </li>
      
      
      
      <li class="nav-item ">
      <a class="nav-link " href="/bio/">bio</a>
      </li>
      
      
      
      <li class="nav-item ">
      <a class="nav-link " href="/music/">music</a>
      </li>
      
      
      
      <li class="nav-item ">
      <a class="nav-link " href="/publications/">publications</a>
      </li>
      
      
      
      <li class="nav-item ">
      <a class="nav-link " href="/projects/">projects</a>
      </li>
      
      
      
      <li class="nav-item active">
      <a class="nav-link active" href="/lab/">lab</a>
      </li>
      
    </ul>

    <ul class="navbar-nav">
      <li class="nav-item">
	      <a rel="me" href="https://aus.social/@charlesmartin" class="nav-link">
		      <i class="fab fa-mastodon"></i></a>
</li>
      <li class="nav-item">
	      <a href="https://pixelfed.au/charlesmartin" class="nav-link">
		      <i class="fa fa-camera"></i></a>
</li>
      <li class="nav-item">
	      <a href="https://github.com/cpmpercussion" class="nav-link">
		      <i class="fab fa-github"></i></a>
</li>
      <li class="nav-item">
	      <a href="https://charlesmartin.bandcamp.com/" class="nav-link">
		      <i class="fab fa-bandcamp"></i></a>
</li>
      <li class="nav-item">
	      <a href="https://scholar.google.no/citations?user=mTlH4G8AAAAJ" class="nav-link">
		      <i class="ai ai-google-scholar-square"></i></a>
</li>
    </ul>
  </div>
  </div>
</nav>
<!-- unlisted links -->
<a rel="me" href="https://mastodon.acm.org/@charlesmartin"></a>


    <div class="p-5 mb-4 rounded-3 bg-transparent text-light text-center d-flex align-items-center">
      <h1 class="heading-logo display-2 mx-auto">Charles Martin</h1>
    </div>

    

	  <div class="container bg-white rounded">	   
	    <main class="p-4" aria-label="Content">
		    <article>
  
  <div class="my-4 py-4">
    <h1 class="display-5">SMCCLAB: Sound, Music and Creative Computing Lab</h1>
  </div>
  

  

  <p>The Sound, Music and Creative Computing Lab is part of the  <a href="https://comp.anu.edu.au">School of Computing</a> at the Australian National University.</p>

<p>The goal of the lab is to create <strong>new kinds of musical instruments</strong> that <strong>sense</strong> and <strong>understand</strong> music. These instruments will actively respond during performances to assist musicians.</p>

<p><img src="/assets/images/performing/metatone-hands-header.jpg" alt="Performing on touchscreens and percussion"></p>

<p>We envision that musical instruments of the future will do more than react to musicians. They will <strong>predict their human player’s intentions</strong> and <strong>sense the current artistic context</strong>. Intelligent instruments will use this information to shape their sonic output. They might seamlessly add expression to sounds, update controller mappings, or even generate notes that the performer hasn’t played (yet!).</p>

<p>The idea here is not to put musicians out of work. We want to create tools that allow <strong>musicians</strong> to reach the highest levels of <strong>artistic expression</strong>, and that assist <strong>novice users</strong> in experiencing the <strong>excitement and flow of performance</strong>. Imagine an expert musician recording themselves on different instruments in their studio, and then performing a track with a live AI-generated ensemble, trained in their style. Think of a music student who can join their teachers in a jazz combo, learning how to follow the form of the song without worrying about playing wrong notes in their solo.</p>

<p>We think that combining music technology with AI and machine learning can lead to a plethora of new musical instruments. <strong>Our mission is to develop new intelligent instruments, perform with them, and bring them to a broad audience of musicians and performers</strong>. Along the way, we want to find out what intelligent instruments mean to musicians, to their music-making process, and what new music these tools can create!</p>

<p>Our work combines three cutting edge fields of research:</p>

<ul>
  <li>
<strong>Expressive Musical Sensing</strong>: Understanding how music is played and what performers are doing. This involves hardware prototyping, creating new hyper-instruments, and applying cutting-edge sensors.</li>
  <li>
<strong>Musical Machine Learning</strong>: Creating and training predictive models of musical notes, sounds, and gestures. This includes applying techniques symbolic music generation, to understand scores and MIDI data,  and music information retrieval to “hear” music in audio data.</li>
  <li>
<strong>Musical Human-Computer Interaction</strong>: Finding new ways for predictive models to work with musicians, and to analyse the musical experience that emerges.</li>
</ul>

<h2 id="current-lab-members">Current Lab Members</h2>

<ul>
  <li>
<a href="">Minsik Choi</a> (PhD Researcher)</li>
  <li>
<a href="https://yichenwangs.github.io/yichen/">Yichen Wang</a>  (PhD Researcher)</li>
  <li>
<a href="https://www.linkedin.com/in/xinlei-niu-544ab6216/?originalSubdomain=au">Xinlei Niu</a> (PhD Researcher)</li>
  <li>
<a href="http://bschuetze.xyz">Brent Schuetze</a> (Research Assistant)</li>
</ul>

<p>(<a href="/lab/grads/">list of graduated students</a>)</p>

<h2 id="lab-pages">Lab Pages</h2>

<ul>
  <li><a href="/lab/grads/">Alumni and graduated students</a></li>
  <li><a href="/lab/milestones/">Milestone expectations for PhD/MPhil students</a></li>
  <li><a href="/blog/2021/08/14/how-to-start-research-writing">Getting started with research writing</a></li>
  <li><a href="/blog/2020/08/09/student-project-repository">Presenting a software project</a></li>
  <li>
<a href="">So you want to do a PhD/Masters/Honours/Project…</a> (todo…)</li>
  <li>
<a href="">Expectation setting for PhD/MPhil students</a> (todo…)</li>
</ul>

<h2 id="projects">Projects</h2>

<p>Here’s some information about some musical AI projects we have worked on.</p>

<h3 id="intelligent-musical-prediction-system-imps">Intelligent Musical Prediction System (IMPS)</h3>

<p>The <a href="/imps/">Intelligent Musical Prediction System (IMPS)</a> is a system for connecting musicians and interface developers with deep neural networks. IMPS connects with any musical interface or software using open sound control (OSC) and helps users to record a dataset, train a neural network and interact with it in real-time performance. See it in action in demo video below:</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/Kdmhrp2dfHw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div>
<!-- https://youtu.be/Kdmhrp2dfHw -->

<h3 id="physical-musical-rnn">Physical Musical RNN</h3>

<p>This project was to develop a physically encapsulated musical neural network. The box contains a Raspberry running a melody-generating recurrent neural network that continually composes music. You can adjust the sound, tempo, the ML-model used, and the “randomness” of the chosen samples to guide the music making process.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/2RDVyOTRAj4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div>
<!-- https://youtu.be/2RDVyOTRAj4 -->

<h3 id="phaserings-for-ml-connected-touchscreen-ensemble">PhaseRings for ML-connected touchscreen ensemble</h3>

<p>PhaseRings is a touchscreen instrument that works with an ML-connected ensemble. A server tracks the four performer’s improvisations and adjusts their user interface during the performance to give them access to different notes and sounds on their screens.</p>

<!-- ![Musicians performing on ML-enhanced touchscreen instruments](/assets/images/teaching/ipad-ensemble.jpg) -->

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/aDEQMLwd8ok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div>
<!-- https://youtu.be/aDEQMLwd8ok -->

<h3 id="self-playing-sensor-driven-guitars">Self-playing, sensor-driven guitars</h3>

<p>This installation of six self-playing, sensor-driven guitars was developed as part of collaborations at the University of Oslo’s RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion. Each guitar uses a <a href="https://bela.io">Bela</a> embedded computer to generate sounds from a speaker driver attached to the guitar body. A distance sensor track the movement of listeners in the environment and the guitars use a firefly synchronisation algorithm to phase in and out of time.</p>

<p><img src="/assets/images/performing/bela-guitars2.jpg" alt="Self-playing sensor-driven guitars"></p>

<h3 id="embodied-predictive-musical-instrument-empi">Embodied Predictive Musical Instrument (EMPI)</h3>

<p>The EMPI is a minimal electronic musical instrument for experimenting
with predictive interaction techniques. It includes a single physical
input (a lever) and a matching physical output, built-in speaker, and
a Raspberry Pi for sound synthesis and ML computations.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/tvgqxmHr9wU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div>

<!-- https://youtu.be/tvgqxmHr9wU -->

<!--

Summer project goals:
Team project: Create an AI-enhanced band.
Need ML-interactions for each performer in a small band (e.g., Jazz combo: bass, drums, piano, and sound engineer).
Sound engineer: Apply techniques of Intelligent Music Production to assist a sound engineer in making a live or recorded mix of a band. This could include mic-placement, volume, EQ, panning, and application of audio effects.
Piano: Need to use a cutting model such as Music Transformer to alternate between playing a song's melody, comping, and soloing. We will need to study data of each type of performance.
Drums: We need to study drummer's playing styles to apply expression to stable drumset loops and introduce variations, fills, and stylistic changes.
Bass: We need to develop 

Individual Projects:
- Enhance aspects of IMPS (Intelligent Music Prediction System)
- Develop new MIR metrics for application in future collaborations. How do we know that generated signals are good?

-->

</article>


	    </main>
	  </div>
    <footer class="text-light text-center py-5">
  © Charles P. Martin 2022
  <a rel="me" href="https://mastodon.acm.org/@charlesmartin"></a>
</footer>

	  
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha2/dist/js/bootstrap.bundle.min.js" integrity="sha384-qKXV1j0HvMUeCBQ+QVp7JcfGl760yU08IQ+GpUo5hlbpg51QRiuqHAJz8+BrxE/N" crossorigin="anonymous"></script>

  </body>
</html>
