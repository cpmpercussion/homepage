<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://charlesmartin.au/feed.xml" rel="self" type="application/atom+xml" /><link href="https://charlesmartin.au/" rel="alternate" type="text/html" /><updated>2026-02-08T12:14:35+00:00</updated><id>https://charlesmartin.au/feed.xml</id><title type="html">Charles Martin</title><subtitle>Charles Martin is a computer scientist and musician specialising in music technology, creative AI, and human-computer interaction.
</subtitle><entry><title type="html">On hosting NIME2025 in Canberra</title><link href="https://charlesmartin.au/blog/2026/02/05/on-hosting-nime2025-in-canberra" rel="alternate" type="text/html" title="On hosting NIME2025 in Canberra" /><published>2026-02-05T23:03:00+00:00</published><updated>2026-02-05T23:03:00+00:00</updated><id>https://charlesmartin.au/blog/2026/02/05/on-hosting-nime2025-in-canberra</id><content type="html" xml:base="https://charlesmartin.au/blog/2026/02/05/on-hosting-nime2025-in-canberra"><![CDATA[<p>In June 2025, I hosted the <a href="https://nime.org">New Interfaces for Musical Expression (NIME)</a> conference at ANU in Canberra, and served as a general chair along with Pia van Gelder from the ANU School of Art and Design.
In this post I want to talk a bit about the kind of conference we produced, discuss (brag about) what we achieved and thank everybody who contributed. 
Along the way, I have images from a few conference highlights.</p>

<p>I’ve been a NIMEr since 2010 when I presented my first academic paper and performance at UTS in Sydney. 
As NIME is my favourite academic community it’s been a career goal to host everybody at my home institution in Canberra.</p>

<p>NIME is a truly interdisciplinary community of artist/scholars who present work as scientific papers, performances, installations and demonstrations. 
While I’m deeply invested in paper writing as an academic, I find something really special about NIME’s music program. 
The level of risk and commitment required to present research in performance form gives that knowledge a level of compelling authenticity. 
At a time when the validity of written research and peer review is threatened, the embodied communication of research through artistic tracks seems all the more relevant.</p>

<p>I’m so proud of what we have achieved in NIME2025. 
You can find the whole program archived in the conference website <a href="https://nime.org/web_archive/2025/">here</a> and <a href="https://nime-conference.github.io/nime2025-website/">here</a>. Video from every session and concert is available in this <a href="https://www.youtube.com/playlist?list=PLF2XENwaPt72U-yGKI30SxxLNRCR-Tot1">Youtube Playlist</a> and to download here <a href="https://doi.org/10.5281/zenodo.17972009">https://doi.org/10.5281/zenodo.17972009</a>.</p>

<h2 id="attendees">attendees</h2>

<p>Our conference attendance and paper/music acceptances were reported in the <a href="https://nime.org/web_archive/2025/sessions/plenary-town-hall.html">town hall meeting (link)</a>, but here’s a short recap.</p>

<p>With the conference held in Australia, the cost of travel affected potential attendance. We worked really hard to provide a great hybrid setup with synchronous and asynchronous participation for those who wanted to attend without travelling.</p>

<p>In the end we had around 50 remote attendees and 149 in person. I wrote “around” because some folks switched attendance at the last minute due to personal travel arrangements or worries about border crossings.</p>

<h2 id="papers">papers</h2>

<p>NIME typically has around 100 accepted papers (presented either as seminar or poster) and we had 96 this year.
Running a paper program means running submission, peer review, camera ready submissions, and (finally) programming the conference sessions. 
Wrangling reviewers, meta-reviewers, and authors to all participate in this process is a huge job and lasts from the paper submission deadline up until the start of the conference. 
<a href="https://hitmuri.net">Florent Berthaut</a> and <a href="https://pure.itu.dk/en/persons/doga-buse-cavdir/">Doga Cavdir</a> (paper chairs) and <a href="https://www.yichenwang.io">Yichen Wang</a> (poster chair) did an incredible job throughout.</p>

<p>I worked closely with these three committee members to make sure we had the best and clearest guidance for <a href="https://nime.org/web_archive/2025/submissions/">submission</a> and <a href="https://nime.org/web_archive/2025/reviewing/">review</a> yet. 
Clear instructions are an often overlooked measure for inclusion as some potential great contributors (for whatever reason) may not be confident in  the secret knowledge of how to write and review conference papers. 
I think our efforts paid off in terms of the quality of reviews that our community wrote and received.</p>

<h2 id="music">music</h2>

<p>This year, we had 44 accepted musical works (performances, installations and remote presentations). 
The music program is an area where local production knowledge is super important given that feasibility and technical support are crucial for how the music submissions re presented. 
I felt very lucky to have Sophie Rose, Nicole Carroll and Jos Mulder as music chairs, all exemplary musicians and experienced concert producers. 
As it turns out, even more help was needed, so Pia van Gelder, Alec Hunter and I jumped in towards the end of the peer review process to help with feasibility and programming of accepted works with Pia and Alec looking particularly at installations and me on concerts. 
During the conference we had additional help from Chloë Hobbs, Richard Johnson, Sophie Edwards, Sam James, and Adrian Rytir.</p>

<p>I want to single out Sophie and Nicole here for MCing and producing so many of the concerts while also continuing the fine tradition of NIME-themed standup comedy to cover last minute technical issues!</p>

<p>Where paper chairing goes up until the start of the conference, music chairing continues at high intensity up to the actual concerts and installation showings as the artistic works require technical riders, input lists, stage plots, bumping in, rehearsal, installation, testing, (performance/showing) and bumping out.</p>

<p>While the demands here can be quite extreme, the personal rewards are high to see concerts come together. 
ANU is reasonably unique in having (multiple!) high quality, flexible artistic venues available in the middle of campus. 
This was a first time for me to use our Drama Theatre and Recording Studio for concerts, and (approximately) the one millionth time I put on works in our Big Band Room (where I had spent a lot of time as an undergrad 20 years ago). 
Seeing new and exciting NIME artistic research in these spaces was just the best.</p>

<h2 id="workshops-and-extras">workshops and extras</h2>

<p>The workshops program was in the capable hands of Minsik Choi and Alon Ilsar who managed the review process and organisation of workshops expertly. 
Traditionally, workshops are on the first day of NIME so they set the tone for how our community interacts and set up NIME newbies with conference buddies for the rest of the event. 
We held our workshops around our computer science, maths, and engineering classrooms in the College of Systems and Society. 
This was a big moment for Sandy Ma and our team of volunteers who needed to meet all the conference attendees and help them get where they needed to go. 
It was so great to see new ideas and sub-communities forming at these events, as well as NIMErs trying out new systems and learning new approaches.</p>

<p>I want to shout out to Samantha Bennet of the ANU School of Music who provided an <em>extra</em> event: a live demonstration of the Fairlight CMI. 
We held this event in the (packed) recording studio (as it turns out NIME has more than a few vintage synth fans). 
Working with antique hardware is a huge challenge and Sam interspersed a live demo with discussion on the Firelight’s history (connections to ANU!), technology, ideology. 
Again, this kind of presentation just felt so authentic to our work as music technology researchers. 
There was a tense moment during the talk when it seemed like loading some samples wasn’t going to happen (with the Fairlight emitting loud and troubling BEEPs) and a cheer went out when at the last moment it worked and came through the speakers.</p>

<p>As NIMErs, a lot of our work revolves around not just what is <em>possible</em> but what is reliably going to work on stage. 
The workshops and our Fairlight demo foreground the frustrating but rewarding process of <em>making things work together</em> that is part of so much of our research.</p>

<h2 id="hybrid">hybrid</h2>

<p>Running a hybrid conference is certainly tough. 
I’ve done it twice now (<a href="https://charlesmartin.au/blog/2023/07/20/hybrid-conference">OzCHI 2022</a>, and now NIME 2025) and although the effort is high I felt the impact was worthwhile.</p>

<p>Our <a href="https://charlesmartin.au/blog/2023/07/20/hybrid-conference">hybrid approach</a> prioritised a great experience for folks in the lecture hall and a smooth stream output for viewers. All of our seminar paper presentations were required to be live with Q&amp;A.
Our remote poster presenters submitted pre-recorded videos which were played on a loop during the poster sessions.</p>

<p>In practice, the live paper sessions were effective in the room and online with genuine engagement between speakers and viewers. 
The poster sessions were well produced with great videos but perhaps missed the excitement of in-person demonstration.</p>

<p>Much credit should go to the paper chairs (Florent Berthaut and Doga Cavdir) who wrangled the timetable to make sure every remote author had an appropriate time to present. 
Poster chair Yichen Wang managed the stressful process of bringing together all the pre-recorded poster videos.</p>

<p>During any hybrid session, an expert is needed to make sure that the presenters are cued up, cameras pointing the right way, microphones switched on and streams streaming. 
Our expert was Albert-Ngabo Niyonsenga who has my eternal gratitude for taking on this hard task not just during the paper sessions but at each of the 7 concerts!</p>

<p>All of this was a lot of effort, but was it worth it? 
I think so. 
Many of the remote presenters were graduate students who were counting on conference publications as part of their studies but couldn’t afford the travel. 
A remote participant saves themselves the high cost of international travel and they save the planet the carbon cost of intercontinental air travel, this means that effective hybrid arrangements are a measure to address inclusion and sustainability.</p>

<h2 id="proceedings">proceedings</h2>

<p>We were the first NIME in a while to publish paper proceedings in advance of the conference<sup id="fnref:paper-proceedings"><a href="#fn:paper-proceedings" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. 
After the accepted paper deadline, there is an enormous amount metadata, formatting, and file checking needed to create accurate proceedings entries. 
Sam Trolland excelled in this role, taking over the publication process from the paper, music, and general chairs and communicating with the NIME Proceedings Officer Stefano Fasciani.</p>

<p>Music proceedings took a little longer, but again, Sam managed the metadata and we had formatting help from Albert-Ngabo Niyonsenga, Yichen Wang, and a bit of last minute effort from me to get things to the finish line. 
While the music proceedings took some time, the formatting quality has improved and we have sent our process and template improvements forward to the 2026 team.</p>

<p>Proceedings are so important for establishing the credibility of NIME and I think it’s crucial that we commit to timely publication. 
As an independent scholarly organisation we have the advantage of not having to wait for anybody else’s permission to publish the PDFs but the responsibilities of metadata checking and communicating with the very last uncommunicative author can be overwhelming.</p>

<h2 id="budget">budget</h2>

<p>It’s tough to make ends meet in a conference these days at all scales. Any event outside a <em>very</em> small conference is likely to have costs that need to be met. 
For NIME, the main fixed costs come from securing a big enough hall for the single track paper events and venues (and technical staff) for the two concerts that need to take place each day.
This year, we managed to make a profit, largely on the basis of tremendous amounts of volunteer work from the committee and the good engagement from remote participants.</p>

<p>I hope that our example can encourage some of my colleagues that, yes, it is still possible to not lose money on a NIME-shaped conference and to have a (reasonably) good time doing so. 
The community needs established scholars to volunteer their time and welcome folks to their institutions—but we shouldn’t be also asking them to take a financial loss.</p>

<h2 id="thanks">thanks</h2>

<p>I’ve mentioned them all above, but I want to thank all the committee members for NIME 2025 again:</p>

<ul>
  <li>General Chairs: Charles Martin, Pia van Gelder</li>
  <li>Paper/Poster Chairs: Florent Berthaut, Doga Cavdir, Yichen Wang</li>
  <li>Music Chairs: Sophie Rose, Nicole Carroll, Jos Mulder</li>
  <li>Workshop Chairs: Minsik Choi, Alon Ilsar</li>
  <li>Accessibility: Alexander Hunter</li>
  <li>Hybrid: Albert-Ngabo Niyonsenga</li>
  <li>Proceedings: Sam Trolland</li>
  <li>Student Volunteers: Sandy Ma</li>
  <li>Social Media: Patrick Hartono</li>
</ul>

<p>I especially want to shout out to the PhD students on the committee and members of my ANU SMCClab who were balancing their volunteer work with studies:</p>

<p>ANU SMCClab (sound, music and creative computing lab):</p>

<ul>
  <li>Yichen Wang (posters!)</li>
  <li>Sandy Ma (ambassadors/volunteers!)</li>
  <li>Minsik Choi (workshops!)</li>
  <li>Albert Niyonsenga (hybrid STREAMING!)</li>
</ul>

<p>Monash Sensilab:</p>

<ul>
  <li>Sam Trolland (proceedings!)</li>
</ul>

<p>And thank you to all the attendees for showing up and bringing your wonderful research and artistic creativity!</p>

<h2 id="special-moments">special moments</h2>

<p><img src="/assets/blog/2025/NIME2025-selected-01.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-02.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-03.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-04.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-05.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-06.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-07.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-08.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-09.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-10.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-11.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-12.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-13.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-14.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-15.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-16.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-17.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-18.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-19.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-20.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-21.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-22.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-23.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-24.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-25.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-26.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-27.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-28.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-29.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-30.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-31.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-32.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-33.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-34.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-35.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-36.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-37.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-38.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-39.jpg" alt="" />
<img src="/assets/blog/2025/NIME2025-selected-40.jpg" alt="" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:paper-proceedings">
      <p>In ye olden days, the paper proceedings would have been finalised and <em>printed</em> in a <em>book</em> for conference attendees to purchase as part of the registration fee. So I can’t quite say we were the <em>first</em> to be ready in advance of the conference, but definitely the first <em>in a while</em>. <a href="#fnref:paper-proceedings" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[In June 2025, I hosted the New Interfaces for Musical Expression (NIME) conference at ANU in Canberra, and served as a general chair along with Pia van Gelder from the ANU School of Art and Design. In this post I want to talk a bit about the kind of conference we produced, discuss (brag about) what we achieved and thank everybody who contributed. Along the way, I have images from a few conference highlights.]]></summary></entry><entry><title type="html">Amazing weekend of performances at SoundOut 2026</title><link href="https://charlesmartin.au/blog/2026/02/01/soundout-2026" rel="alternate" type="text/html" title="Amazing weekend of performances at SoundOut 2026" /><published>2026-02-01T00:00:00+00:00</published><updated>2026-02-01T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2026/02/01/soundout-2026</id><content type="html" xml:base="https://charlesmartin.au/blog/2026/02/01/soundout-2026"><![CDATA[<p>I had an amazing weekend performing at <a href="https://dhg.anu.edu.au/event_post/soundout-2026/">SoundOut 2026</a> at the ANU Drill Hall Gallery performing with some incredible Australian and international musicians. SoundOut is described as “one of the gateway exploratory music-art events in Australia” and a gem in Canberra’s yearly music schedule.</p>

<p>I was lucky enough to perform wonderful sets with Jesse Twomey, Yichen Wang, Ben Carey, Jamie Lambert, Peter Bruun, Rhys Butler, Phil Durrant, Laura Altman and Nicci Haynes. It was wonderful to perform with these folks including some I hadn’t met before and some very old friends!</p>

<p>Especially cool this year to show off two new instruments designed in the ANU SMCClab with Yichen Wang. We have been working hard in the lab to design new ways to build musical instruments involving AI and machine learning technologies.</p>

<p>Huge thanks to Richard Johnson for inviting me to be a part of this SoundOut and congratulations on another amazing festival!</p>

<p>Here’s a few images…</p>

<p><img src="/assets/blog/2026/soundout2026-1.jpg" alt="Yichen Wang and Jesse Twomey setting up for performance." /></p>

<p><img src="/assets/blog/2026/soundout2026-2.jpg" alt="My instrument throughout the festival: IMPSYpi intelligent instrument platform plus Roland S-1 synth and Keith McMillen QuNeo" /></p>

<p><img src="/assets/blog/2026/soundout2026-3.jpg" alt="blinky lights" /></p>

<p><img src="/assets/blog/2026/soundout2026-4.jpg" alt="Ben Carey and Rhys Butler preparing for our ensemble performance." /></p>

<p><img src="/assets/blog/2026/soundout2026-5.jpg" alt="Yichen Wang's performance rig" /></p>

<p><img src="/assets/blog/2026/soundout2026-6.jpg" alt="Quintet in performance" /></p>

<p><img src="/assets/blog/2026/soundout2026-7.jpg" alt="Large group packing up at the end of the festival." /></p>]]></content><author><name></name></author><category term="note" /><category term="projects" /><summary type="html"><![CDATA[I had an amazing weekend performing with intelligent musical instruments at SoundOut with incredible Australian and international musicians]]></summary></entry><entry><title type="html">SMCClablive #1 concert</title><link href="https://charlesmartin.au/blog/2025/11/24/smcclablive-1" rel="alternate" type="text/html" title="SMCClablive #1 concert" /><published>2025-11-24T00:00:00+00:00</published><updated>2025-11-24T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2025/11/24/smcclablive-1</id><content type="html" xml:base="https://charlesmartin.au/blog/2025/11/24/smcclablive-1"><![CDATA[<p>The first of our smcclablive concerts was held at the Peter Karmel Building Big Band Room on 7 November 2025. Works were presented from researchers and students in the ANU SMCClab with guests Charlie Roberts from Worcester Polytechnic Institute (USA) and Ben Swift from the ANU School of Cybernetics.</p>

<p>The video from our concert is available <a href="https://youtu.be/S_JuZUTmwbk">here (YouTube)</a> and below.</p>

<p>It was so wonderful to present new music with students and colleagues. We were thrilled to have a busy audience of interested folks at the intersections of art, music, and computing in Canberra.</p>

<p>We plan to run smcclablive as an end-of-semester event every six months featuring visiting artists and researchers from Australia and around the world.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/S_JuZUTmwbk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div>

<h2 id="program">Program</h2>

<ol>
  <li>Yijia Wang - Objects in VR</li>
  <li>Ben Swift - Live Code</li>
  <li>Sandy Ma and Suhani Narang - Webcam Duet</li>
  <li>Charles Martin and Yichen Wang - Beep Duet</li>
  <li>Charlie Roberts - Live Code in Mutter (<strong>world premiere performance!</strong>)</li>
</ol>

<h2 id="images">Images</h2>

<p><img src="/assets/blog/2025/2025-smcclablive-1-1.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-2.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-3.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-5.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-6.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-7.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-8.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-9.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-10.jpg" alt="" /></p>

<p><img src="/assets/blog/2025/2025-smcclablive-1-11.jpg" alt="" /></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A wrapup of events at our SMCClablive #1 in early November 2025 with live coder Charlie Roberts (USA) and researchers from the ANU SMCClab]]></summary></entry><entry><title type="html">Concert with Andromeda is Coming and Artificially Intelligent Friends</title><link href="https://charlesmartin.au/blog/2024/05/31/andromeda-artificial-friends" rel="alternate" type="text/html" title="Concert with Andromeda is Coming and Artificially Intelligent Friends" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2024/05/31/andromeda-artificial-friends</id><content type="html" xml:base="https://charlesmartin.au/blog/2024/05/31/andromeda-artificial-friends"><![CDATA[<p>Andromeda is Coming, the exploratory music project of (me!) Charles Martin and Alec Hunter recently developed a concert of new works all featuring research into intelligent (or AI-enabled) musical instruments. Incorporating new perspectives in our work, we invited two members of my ANU SMCClab research lab, Yichen Wang and Sandy Ma to join us for a 60-minute concert of live audio-visual performances.</p>

<p>Our concert took place on Wednesday 29 May 2024 at the Big Band Room, ANU School of Music. The whole concert can be viewed <a href="https://youtu.be/86yEZquWsPs">here</a> and below.</p>

<p>It was an important moment for our lab to put together a whole concert of music, not just the typical 10-minute conference demo, and to invite the community to see it. As always, it was a privilege to share the stage with Alec Hunter, Yichen Wang, and Sandy Ma. I want to thank Craig Greening for helping with sound and Eric Byler for taking photos and video!</p>

<p><strong>Update:</strong> the <a href="https://systems.anu.edu.au/news/whats-sound-duet-humans-and-ai-concert">College of Systems and Society has posted an article about our work at this concert!</a></p>

<h2 id="program">Program</h2>

<ol>
  <li><strong>From the Earth</strong> (Charles Martin and Alec Hunter) Improvisation for AI-assisted synths and soundscapes on AUM (iPad) and Ableton Live (laptop).</li>
  <li><strong>Unspoken</strong> (Yichen Wang and Sandy Ma) Augmented-reality-mediated collaboration for synthesiser and computer music drawing interface.</li>
  <li><strong>Cubes and Freak</strong> (Yichen Wang and Charles Martin) Improvised with Augmented reality musical instrument and AI-assisted MicroFreak Synthesiser.</li>
  <li><strong>Touching Wires</strong> (Sandy Ma) Improvisation for <a href="https://bela.io">Bela</a> sensor quilt musical instrument and live video processing.</li>
  <li><strong>Off the Shelf</strong> (Yichen Wang) Improvisation for AI-powered Volca FM synthesiser and OP-1 synthesiser.</li>
  <li><strong>Into the Sun</strong> (Charles Martin and Alec Hunter) Improvisation for AI-assisted synths and soundscapes on AUM (iPad) and Ableton Live (laptop).</li>
</ol>

<h2 id="video">Video</h2>

<p><a href="https://youtu.be/86yEZquWsPs">video here</a></p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container"><iframe src="https://www.youtube.com/embed/86yEZquWsPs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div>

<h2 id="images">Images</h2>

<p><img src="/assets/blog/2024/2024-05-29-concert-1.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-2.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-3.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-4.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-5.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-6.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-7.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-8.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-9.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-10.jpg" alt="" /></p>

<p><img src="/assets/blog/2024/2024-05-29-concert-11.jpg" alt="" /></p>

<h2 id="concert-details">Concert Details</h2>

<p><img src="/assets/blog/2024/2024-aicaif-concert.jpg" alt="" /></p>

<p>A 60-minute performance of new music for new AI musical instruments featuring Alec Hunter, Charles Martin, Yichen Wang and Sandy Ma.</p>

<p>Andromeda is Coming is the improvised music project of Charles Martin and Alec Hunter dedicated to exploring new sounds and music making technologies in long-form musical performances.</p>

<p>For this performance, Andromeda adopt artificial intelligence and augmented reality musical instruments from the ANU Sound, Music and Creative Computing Lab. These include Charles Martin’s IMPSY system for building intelligent musical instruments and Yichen Wang’s “Cubing Sound” augmented reality musical instrument. These new musical instruments will be reflected by real-time interactive visualisation.
We ask what new sounds and new music might be created with these new technologies, and what AI and virtual musical performances will look like?</p>

<p>Poster Image Credit: Sandy Ma</p>

<p><a href="https://music.cass.anu.edu.au/events/andromeda-coming-and-artificially-intelligent-friends">ANU School of Music Event Link</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Bringing back Andromed is Coming with new collaborators and new AI music systems.]]></summary></entry><entry><title type="html">SMCClab projects on gesture, collaboration and intelligence</title><link href="https://charlesmartin.au/blog/2023/08/26/projects-in-24" rel="alternate" type="text/html" title="SMCClab projects on gesture, collaboration and intelligence" /><published>2023-08-26T00:00:00+00:00</published><updated>2023-08-26T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2023/08/26/projects-in-24</id><content type="html" xml:base="https://charlesmartin.au/blog/2023/08/26/projects-in-24"><![CDATA[<p>I’ve recently been thinking about projects my lab has been working on and how
to focus our work in future to take advantage of collaboration and
knowledge-sharing within my group.</p>

<p>All of my group’s work is related to <a href="https://comp.anu.edu.au/courses/laptop-ensemble/">sound and music
computing</a>, but we broadly
work in three themes: gesture, collaboration, and (computational) intelligence.
Projects we have been working on tend to sit on one or across two o these
themes, for example:</p>

<ul>
  <li>
    <p><strong>Spatial Interaction (gesture x collaboration)</strong>: Creating authentic spatial
musical apps that support interaction in AR/VR computers and novel sensors.
We have experimented with new freehand gestures in AR and created new kinds
of artistic performances. This work has now led to collaborative music
systems in AR.</p>
  </li>
  <li>
    <p><strong>Generating Creative Gestures (gesture x intelligence)</strong>: My work with the
<a href="">EMPI</a> and <a href="/projects/imps/">IMPS</a> systems focussed on
generating creative gestural data. Benedikte Wallace’s work on generative
dance expanded these and engaged with full-body motion. Xinlei Niu has
focussed on directly generating digital audio.</p>
  </li>
  <li>
    <p><strong>Guiding Collaborative Performance (collaboration x intelligence)</strong>: This
research aims to create new ways for groups to make music together. We
created touchscreen musical instruments that communicate interactions over a
network and ways to remotely adjust these interfaces.  We plan to create AI
models of performance and find ways to guide improvisors towards new musical
states. This work follows the research projects I did with <a href="/projects/metatone/">Ensemble
Metatone</a> and <a href="/projects/ensemble-evolution/">Ensemble Evolution</a> on collaborative touchscreen
performance.</p>
  </li>
  <li>
    <p><strong>Intelligent Instruments (gesture x collaboration x intelligence)</strong>:
Intelligent instruments predict human musical interactions to help them
create music. This work involves encapsulating machine learning models of
creative interaction into a playable instrumen. Our prototypes include
<a href="">EMPI</a>, a portable musical robot that responds to performances with a
1-dimensional musical interface, self-playing iPads and AI laptop ensembles.
The collaboration here can be between the performer and instrument as well as
with other musicians or artists. This project is hard because we have to
understand models of gesture and apply them in different interactive systems.
One particularly interesting approach has been to create physical hardware
instruments with Raspberry Pi or Bela. Another way might be to create spatial
systems in AR/VR.</p>
  </li>
</ul>

<p>The point of writing the above is that, starting from now (2023) and at least
for a year or two, I’m only going to support projects that fit within the above
four meta-projects or are very clearly aligned with the themes of gesture,
collaboration and intelligence. These are hard problems, and we need to create
instruments that can be deployed and creatively applied in performance to make
an impact. Keeping a focus on a fairly small number of themes will help build
capacity and a culture of sharing within my lab.</p>]]></content><author><name></name></author><category term="note" /><category term="projects" /><summary type="html"><![CDATA[I'm going to focus on spatial interaction, generating creative gestures, intelligent instruments, and collaborative performance.]]></summary></entry><entry><title type="html">One-person hybrid conference toolkit</title><link href="https://charlesmartin.au/blog/2023/07/20/hybrid-conference" rel="alternate" type="text/html" title="One-person hybrid conference toolkit" /><published>2023-07-20T00:00:00+00:00</published><updated>2023-07-20T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2023/07/20/hybrid-conference</id><content type="html" xml:base="https://charlesmartin.au/blog/2023/07/20/hybrid-conference"><![CDATA[<p>Hybrid events are <strong>hard</strong>, even more now that they are kind of <em>expected</em>,
especially within the academic community. This post is about my one-person
hybrid setup.</p>

<p>Last year I was part of the organising team for OzCHI 2022 in Canberra, a
single track conference. Here’s approached doing <em>hybrid</em> aspects of the
conference right, or as well as possible, within a very small budget<sup id="fnref:budget"><a href="#fn:budget" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p>

<p><img src="/assets/blog/2023/streaming-hybrid.jpg" alt="my hybrid prduction setup at the Shine Dome, Canberra" /></p>

<p>The image shows a remote talk, and Restream Studio’s production interface on my laptop. (This was a great talk!)</p>

<p>Here’s a few principles for the event:</p>

<ul>
  <li>make the experience easy and inclusive for hybrid attendees</li>
  <li>use our own equipment, don’t hire a production company</li>
  <li>have high quality audio and video</li>
  <li>keep the focus on attendees in the room</li>
</ul>

<p>The tl;dr here is: we used <a href="https://restream.io">restream.io</a> as a streaming
and production platform, not Zoom. The endpoints were unlisted youtube streams
sent to remote attendees over Slack. We used a dedicated computer for streaming
and an HDMI switcher to grab the signal from presenter’s laptops. It was
helpful to have a good quality camera and microphone connected ot the streaming
computer to capture the speaker at the lectern. The main takeaways are:</p>

<ul>
  <li>browser-based streaming platforms use more resources (CPU + network) than
Zoom and folks aren’t usually prepared for it.</li>
  <li>streaming platforms are <em>much better</em> than Zoom from an audience perspective</li>
  <li>in-person audiences don’t like pre-recorded presentations</li>
</ul>

<h2 id="one-to-many-presentation">One-to-many presentation</h2>

<p>Conference presentations are 1-to-N presentations, so not a great fit for video
conferencing software (e.g., Zoom calls). Listeners want good audio and video
quality and not to be annoyed by software that wants to see you and takeover
your computer. A good solution is a high quality stream to an easy to use
endpoint like YouTube where attendees can view videos in a browser window or on
any device.</p>

<p>One of the difficulties of a hybrid conference is switching between a live view
and remote presenters. Amazingly, there are streaming products out there that
give you a little TV studio for bringing in live guests, switching on shared
screens, and turning cameras on and off all within a web browser (I’m literally
still amazed this all works).</p>

<p>From online teaching I had a subscription to <a href="https://restream.io">restream</a>
and was ready to try out it’s production features for this conference.
Streamyard is another similar option. Restream gives you a browser-based
interface to produce a stream and invite guest presenters. You can choose which
presenter is “live” on the stream and who’s screen is shown. There are neat
options to arrange multiple presenter’s videos side-by-side or next to a shared
screen.</p>

<p>The upside of these streaming systems is that they work <em>really</em>, <em>really</em> well
under many circumstances. The downside is that they aren’t as forgiving as Zoom
and presenters do not usually have experience outside of Zoom or Teams.</p>

<p>It turns out that these browser-based systems tend to use more system and
network resources than Zoom and are a little less reliable at capturing your
screen and audio (depending on your OS). This means that it’s <strong>crucial</strong> to do
some kind of rehearsal or test with presenters to check their setup.</p>

<h2 id="the-setup">The setup</h2>

<p><img src="/assets/blog/2023/streaming-conference-setup.png" alt="a signal diagram for my setup" /></p>

<p>This setup is designed to slot into a regular conference or lecture hall with
HDMI sending video to a big screen and audio to big speakers. We need four
pieces of hardware:</p>

<ol>
  <li>
    <p>An HDMI switcher to go in between the presenter’s laptop and the screen. We used an <a href="https://www.blackmagicdesign.com/au/products/atemmini">ATEM Mini Extreme</a> (already had for teaching). This lets us take an output signal to a dedicated streaming computer, and to easily put the streaming computer’s screen on the big screen during remote presentations.</p>
  </li>
  <li>
    <p>A dedicated straming computer. I used a (now old) 2016 MacBook Pro. A fairly powerful laptop is needed as it will need to handle multiple video streams. The internet connection should be stable and fast.</p>
  </li>
  <li>
    <p>A dedicated camera and microphone to point at the live presenter. I used a GoPro with a Rode VideoMicro microphone. The GoPro has an HDMI output (using the “Media Mod” for this model). A regular/nice webcam could work but I like the extreme wide-angle of the GoPro for capturing presenters who tend to move around a bit. With such a wide-angle this means the camera should basically attached to the speaker’s lectern. The neat little microphone seemed to work well to pick up the presenter and definitely made them feel super pro<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, having the microphone close to and pointed directly at the speaker helps more than the brand of microphone.</p>
  </li>
</ol>

<ol>
  <li><em>Another</em> laptop used by “the producer” (see below).</li>
</ol>

<p>There’s two modes we need to cope with: an in-person talk and a remote talk.</p>

<p>For the <strong>in-person presentation</strong> mode, the HDMI switcher is set to show the presenter’s laptop on the big screen. The HDMI switcher is connected to the streaming laptop (USB) and shows up as a webcam. The switcher’s signal is set to full screen in Restream Studio and the GoPro is set to a small window in the corner. So: remote attendees see and hear both the big screen and the person talking.</p>

<p>For <strong>remote presentation</strong> mode, the streaming laptop’s screen goes to the HDMI switcher (over HDMI) and audio and video are sent to the big screen. The GoPro isn’t sent anywhere. The remote presenter connects to Restream Studio and sends their screen sharing signal and camera.
The in-person audience sees the Restream Studio interface full screen.</p>

<p>The in-person view in remote mode is the only compromise here as the restream studio interface isn’t <em>meant</em> for public viewing (i.e., it has a back-channel chat window and GUI controls for which signal is shown in the stream). One option would be to have a <em>third</em> computer playing the stream from Youtube, but I thought this got a step too complicated. There may be other solutions as well.</p>

<h2 id="the-stream-producer">The stream producer</h2>

<p>To cope with the hybrid setup, we needed at least one person to keep an eye on the studio interface pretty much at all times.</p>

<p><img src="/assets/blog/2023/streaming-restream-studio.png" alt="a screenshot of restream studio" /></p>

<p>Restream and other online streaming production systems let you preview the
cameras and screen sharing of presenters before switching them live on screen.
You can also chat with presenters to make sure they are ready to go.</p>

<h2 id="remote-participants">Remote participants</h2>

<p>It’s hard to be a remote participant and get any feeling of inclusion at a
conference. The best we can do to make the task easy is to communicate early
and regularly with remote speakers and attendees and make sure all information
for watching and giving talks is super clear.</p>

<p>Pre-conference rehearsals goes a long way towards helping speakers feel
included and setting expectations for the streaming system. We found a few bugs
this way and had a chance to test out difficulties with setups, networks, and
laptops. I wrote up some <a href="/misc/remote-presentation-procedure">Instructions for Participants</a> that I shared with all presenters.</p>

<p>As I wrote above, these streaming systems put more load on presenters’
computers than Zoom. This was definitely frustrating for presenters who would
rightly would question why our system seemed to be broken. There’s not much
that can be done on the day of a talk, but if the expectations are set well in
advance, some of these difficulties can be smoothed out.</p>

<p>One efficiency would be to ask for slides well in advance. Restream can host
basic slide decks directly in their interface. This can really help if a remote
presenter’s system is struggling to screen share and stream camera and audio.
Both the presenter and stream producer can control the slide deck in this case,
but animations and videos in the slides aren’t supported.</p>

<p>Some remote participants really wanted to do pre-recorded presentations. I
understand the temptation as these are much easier to produce than a live
hybrid talk. The problem is that pre-recorded talks are terribly boring! Live
attendees regularly walk out of them. I’ve had great experiences with
pre-records in fully-online and asynchronous conferences, but I don’t think
pre-records should be a part of live (and expensive) conference programs.</p>

<h2 id="doing-this-again-better">Doing this again, better.</h2>

<p>I previously had experience with a great <em>virtual</em> conference with
<a href="https://benswift.me/blog/2020/07/15/acmc2020-organising-my-first-virtual-conference/">ACMC2020</a>
with all asynchronous online talks and a really active community watching. The
hybrid setup was much more complex and a huge effort to make it work within a live environment. The
majority of the effort was in managing people, the remote presenters, in-person
session chairs, and helpers who managing the chat and addressing concerns.
Everybody involved can get easily confused and stressed.</p>

<p>It’s possible to get a great result with a one-person setup, but it’s
hard work. Next time, I’d think about:</p>

<ul>
  <li>setting expectations even more clearly with remote participants and collecting more slide decks in advance</li>
  <li>experimenting with a third playback-only computer for showing the remote talks</li>
  <li>training another person to be the stream producer during talks</li>
</ul>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:budget">
      <p>Budget was zero dollars, except, of course, for my time (possibly too much of that), and that I had equipment to hand, e.g., HDMI switcher, GoPro, Microphone, HDMI capture device, two pretty nice laptops, and an existing restream subscription. This is not really a post about cheap tech, but rather avoiding paying excessively for a poor product as I have seen at some very expensive and prestigious conferences. <a href="#fnref:budget" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>Anecdotal evidence can be supplied on request. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="note" /><category term="hybrid," /><category term="conference," /><category term="setup" /><summary type="html"><![CDATA[Hybrid events are hard, even more now that they are kind of expected, especially within the academic community. This post is about my one-person hybrid]]></summary></entry><entry><title type="html">Photoblogging in 2023 like it’s 2008</title><link href="https://charlesmartin.au/blog/2023/01/31/pixelfed" rel="alternate" type="text/html" title="Photoblogging in 2023 like it’s 2008" /><published>2023-01-31T00:00:00+00:00</published><updated>2023-01-31T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2023/01/31/pixelfed</id><content type="html" xml:base="https://charlesmartin.au/blog/2023/01/31/pixelfed"><![CDATA[<p>I’ve been trying to take photos a bit more intentionally a regularly again,
helped by a new (to me) Fujfilm X-T2 camera. I’ve ended up posting some images on <a href="https://pixelfed.au/charlesmartin">pixelfed.au</a> regularly, and hoping to put something quick up most days.</p>

<p><img src="/assets/blog/2023/DSCF2269.jpg" alt="Curtin Trees. Fujfilm X-T2, XF16mmF2.8" /></p>

<p>I guess I’m wondering if it’s possible to <a href="https://en.wikipedia.org/wiki/Photoblog">photoblog</a> in 2023 like it’s 2008…</p>

<p>In the late 2000s, Flickr was the place to be and little bag photos like <a href="https://www.flickr.com/photos/superlocal/304190507/">this</a> sucked me in. Flickr was often about discovery as much as following people, I somehow felt part of a community there even if I wans’t a terribly active participant. I guess Instagram killed my interest in Flickr, but that has just become a doomscrolling timesuck with little interesting discovery.</p>

<p>I guess I could host images here on this site, and I have in the past. Most of the photo posts here were actually imports from <a href="https://en.wikipedia.org/wiki/Posterous">Posterous</a> where the defining feature was emailing a bunch of images to create a gallery post. Given the interest in the fediverse, I’ll see if Pixelfed works. I’ve been using the <a href="https://mastodon.social/@pixelfed/109133606987470446">iOS app</a> which is currently available in an open beta. The app is definitely still a bit buggy, but feels small and calm compared to the big socials.</p>]]></content><author><name></name></author><category term="note" /><category term="photography," /><category term="blogging," /><category term="fediverse" /><summary type="html"><![CDATA[I've been trying to take photos a bit more intentionally a regularly again, helped by a new (to me) Fujfilm X-T2 camera. I've ended up posting some images]]></summary></entry><entry><title type="html">Rebooting Chroma</title><link href="https://charlesmartin.au/blog/2022/08/30/rebooting-chroma" rel="alternate" type="text/html" title="Rebooting Chroma" /><published>2022-08-30T00:00:00+00:00</published><updated>2022-08-30T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2022/08/30/rebooting-chroma</id><content type="html" xml:base="https://charlesmartin.au/blog/2022/08/30/rebooting-chroma"><![CDATA[<p>I’ve just arrived in Te Whanganui-a-Tara/Wellington for the <a href="https://www.acmc2022.com/">Australasian Computer Music Conference</a>, the first in-person edition since 2019, and it’s a good time to reflect on a bit project I’ve undertaken with this community—rebooting our journal: <a href="https://journal.computermusic.org.au/chroma">Chroma: Journal of the Australasian Computer Music Association</a>.</p>

<p>Skipping to the end first, we now have a first issue <a href="https://journal.computermusic.org.au/chroma/issue/view/1">“Rebooting Chroma”</a> with progressively released, open access articles from computer musicians, music technologists, and composers. The journal costs less than 100AUD per year to run and has no costs for authors.</p>

<p><img src="/assets/blog/2022/chroma-volume-38.jpg" alt="Rebooting Chroma, volume 38 cover" /></p>

<p>We use:</p>

<ul>
  <li>a markdown-to-pdf workflow for all articles, the <a href="https://github.com/cpmpercussion/chroma-template">Chroma Template</a></li>
  <li><a href="https://pkp.sfu.ca/ojs/">Open Journal Systems</a> for creating the journal website and reviewing system</li>
  <li>old-school shared hosting with cPanel to host everything as well as the wordpress site for <a href="https://computermusic.org.au">our community</a></li>
</ul>

<p>In this post I want to document a bit of what I have put together to get this journal working. My strong belief is that folks shouldn’t have to pay thousands of dollars to publish journal articles with big publishers: we academics should be running community journals by and for each other where (as peer-reviewers and editors) we can keep an eye on quality as well as support emerging researchers. Community-run journals can run at (almost) zero cost to the researchers and readers they serve by leaning into web-first publishing using free tools.</p>

<h3 id="markdown-to-pdf">Markdown to PDF</h3>

<p>One draw of publishing in a commercial journal is that your output paper <em>looks really nice</em>. This is because the journal pays people to do the production to a certain layout. Well our journal has no money, and I don’t have much time, so we have to rely on computers to help.</p>

<p><em>Luckily</em> it is possible to go from a word-document manuscript to a <a href="https://journal.computermusic.org.au/chroma/article/view/7/8"><em>quite nice looking</em></a> PDF without too much effort using <a href="https://pandoc.org/">pandoc</a> and <a href="https://www.latex-project.org/">latex</a>.</p>

<p>There are
<a href="https://brainbaking.com/post/2021/02/writing-academic-papers-in-markdown/"><em>lots</em></a>
of <a href="https://kieranhealy.org/blog/archives/2014/01/23/plain-text/">blog</a>
<a href="https://opensource.com/article/18/9/pandoc-research-paper">posts</a> out there
about using markdown and pandoc for academic writing. Mostly these are about
using markdown as the starting point to publishing in a traditional journal or
conference using that publisher’s template. The idea here is to let authors use
whatever they want to create a manuscript (usually Microsoft Word) and then I
use pandoc to “magically” turn it into a beautiful PDF.</p>

<p>To do this, I have a <a href="https://github.com/cpmpercussion/chroma-template">template
repository</a> on Github which I
use to create new repos—one for each article. There’s a carefully crafted
latex template file, a demo article, and some scripts to tie everything
together. The workflow works like this:</p>

<ol>
  <li>I run a little script that uses pandoc to extract the text and images from the author’s manuscript: <code class="language-plaintext highlighter-rouge">pandoc -i $1 -o article.md --extract-media .</code></li>
  <li>I paste the output into the template’s <code class="language-plaintext highlighter-rouge">article.md</code> file, and fix up the metadata</li>
  <li>I run <code class="language-plaintext highlighter-rouge">make</code> in the repository to generate the PDF and a bonus HTML version!</li>
</ol>

<p>The good news is that it <em>mostly</em> works.</p>

<p>Normal text is laid out beautifully in the template. Images pretty much work
well although authors get confused about latex’s automatic placement sometimes.
Figure captions don’t seem to work in HTML. Tables tend to need a lot of
reworking to fit into the width of the page generated by latex. References and
citations are a still a bit of an issue. As it turns out, most authors just
have their references in verbatim text in their manuscript. Pandoc supports
more advanced citation practices with bibtex files and citation style language
files, but in practice my job is to take what authors give me and create a PDF.</p>

<p>From my perspective, the production part of editing the journal takes place in a normal code editor like this:</p>

<p><img src="/assets/blog/2022/chroma-template.jpg" alt="editing a chroma article in VSCode" /></p>

<p>Mostly the markdown output from pandoc makes, sense, but I have found that I need to wrap the reference section in a latex <code class="language-plaintext highlighter-rouge">hangparas</code> environment to get them to work correctly:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```{=latex}
\begin{hangparas}{1.5em}{1}
```

(references go here)

```{=latex}
\end{hangparas}
```
</code></pre></div></div>

<p>So far, this is the <em>only</em> latex code I need to paste into the markdown manuscripts, which seems like a pretty big win!</p>

<h3 id="aspiration-an-end-to-end-online-editing-process">Aspiration: an end-to-end online editing process</h3>

<p>I’m still experimenting with a few features of the GitHub template.</p>

<p>For now, I’m happy using the template just in the production stage, but ideally, authors would fork their own template and edit in there directly. I have set up github actions that automagically build and host the compiiled outputs from the template on GitHub pages (e.g., <a href="https://cpmpercussion.github.io/chroma-template/">here (HTML)</a> and <a href="https://cpmpercussion.github.io/chroma-template/article.pdf">here (PDF)</a>).</p>

<p>I’d also like to provide a way to edit an article online. Overleaf works so well for this for LaTeX but it’s not as easy to work out a way to edit markdown and connect to an existing template.</p>

<p>An overall goal would be to have (like in Overleaf) an online workflow for importing a Word document manuscript, editing in markdown and then submitting the automatically generated PDF or HTML file for review. It <em>sort of</em> works with a collection of different tools, but I haven’t had much interest from authors in engaging within the <em>Chroma</em> community in particular.</p>

<h3 id="journal-website">Journal Website</h3>

<p><a href="https://pkp.sfu.ca/ojs/">Open Journal Systems</a> is a venerable and effective way to host a journal as well as manage the reviewing workflow. It’s not very hard to set up in a standard “shared hosting” environment with cPanel. In fact, there’s often a way to install it automatically in cPanel app installer widget.</p>

<p>OJS is fairly detailed and designed for use in journals that are more complicated than our use case (e.g., with different people in charge of review, copy editing, production, etc). I actually only <em>now</em> feel like I understand how the whole publication process is supposed to work, having gone all the way from submission to publication with a couple of papers.</p>

<p>I’m probably most familiar now with the <em>review</em> part of the website as the
second hardest part of journal editing is getting folks to write reviews (the
hardest part is getting authors to revise manuscripts). Email deliverability
can be a tricky issue here. OJS normally likes to send email from PHP using the
server it is running from which, these days, is probably a bad idea. The IP
address of the shared hosting server tends to get spam filtered which makes
getting in touch with reviewers difficult. I switched OJS to use
<a href="https://docs.pkp.sfu.ca/admin-guide/en/email">SMTP</a> and the email server
provided by the our web host. I had to make sure the SPF and DKIM records were
set up correctly, and now I <em>think</em> that the OJS emails are not getting
labelled as “unverified” or being filtered by IP address.</p>

<p>One cool thing about OJS is that it supports multiple output formats for one article, so one the page for one article (e.g., <a href="https://journal.computermusic.org.au/chroma/article/view/7">Sze Tsang’s “Exploring aspects of place through sound mapping”</a>) you can have links to a PDF and HTML version.</p>

<p>Once published, the metadata for articles seems to get picked up by Google Scholar, which, depending on who you ask, might be the only thing that actually matters :-/</p>

<h3 id="it-works">It works!</h3>

<p>There’s complications and difficulties above, but the takeaway should be: it
works! It <em>is</em> possible to do a very low budget community-run journal and
produce pretty good looking PDF output articles. Having tuned the latex
template a bit, the production part is now very quick for a normal article. The
main hassles with running the journal at this point are the normal human ones
of interacting with reviewers and authors!</p>]]></content><author><name></name></author><category term="note" /><category term="publishing," /><category term="journal," /><category term="computer" /><category term="music" /><summary type="html"><![CDATA[I've just arrived in Te Whanganui-a-Tara/Wellington for the Australasian Computer Music Conference, the first in-person edition since 2019, and it's a]]></summary></entry><entry><title type="html">Setting up Tensorflow with Docker</title><link href="https://charlesmartin.au/blog/2021/12/08/TensorFlowDocker" rel="alternate" type="text/html" title="Setting up Tensorflow with Docker" /><published>2021-12-08T00:00:00+00:00</published><updated>2021-12-08T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2021/12/08/TensorFlowDocker</id><content type="html" xml:base="https://charlesmartin.au/blog/2021/12/08/TensorFlowDocker"><![CDATA[<p>For the last five years, I’ve seemed to have a more-or-less annual fight with my Linux workstations over installing CUDA to keep on doing GPU-accelerated musical machine learning research with TensorFlow and Keras.</p>

<p>Each version of TensorFlow requires <a href="https://www.tensorflow.org/install/source#gpu"><em>specific</em> versions of CUDA and cuDNN</a>. The install instructions involve either installing very <a href="https://www.tensorflow.org/install/gpu#install_cuda_with_apt">strange apt packages</a> or finding and downloading binaries from NVIDIA. The whole things seems to take a day to get right. You know it’s bad when you have three or four conflicting gists and Medium articles open just to try to install a library.</p>

<p>While it’s possible to sit on one version for a long time, for some reason or another one part seems to need to be upgraded and then whole system is broken.</p>

<p>Well I say: <em>no more</em>. The <em>suggested</em> way to run TensorFlow is with a <a href="https://www.tensorflow.org/install/docker">docker container</a> and that’s what I’m going to do going forward.</p>

<p>Mostly for my own benefit I’m going to document the setup for going from a new Ubuntu system to being able to run one command to get open a Jupyter notebook server with GPU-connected TensorFlow running. I promise it’s faster than installing CUDA.</p>

<h2 id="install-nvidia-drivers">Install Nvidia Drivers</h2>

<ol>
  <li>Install Ubuntu</li>
  <li>make sure you have a (physical) Nvidia GPU in your computer</li>
  <li>make sure you have installed the Nvidia GPU drivers. This is pretty much the default these days, but here’s the one-liner to install proprietary drivers:</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ubuntu-drivers autoinstall
</code></pre></div></div>

<p>Once you have Nvidia drivers installed, you should be able to run the following command to list your installed GPUs:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>

<p>If the table shows your GPU(s) and driver version, then you’re ready.</p>

<p>While we’re here, consider installing <a href="https://github.com/Syllo/nvtop"><code class="language-plaintext highlighter-rouge">nvtop</code></a>, a convenient command line tool for tracking GPU utilisation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt install nvtop
</code></pre></div></div>

<h2 id="install-docker">Install <code class="language-plaintext highlighter-rouge">docker</code></h2>

<p>Installing Docker on Ubuntu is another one of those too-many-gist-and-medium-article questions.</p>

<p>The <a href="https://stackoverflow.com/questions/45023363/what-is-docker-io-in-relation-to-docker-ce-and-docker-ee">current wisdom (2021)</a> seems to be to install <code class="language-plaintext highlighter-rouge">docker.io</code>, which is a <a href="https://www.collabora.com/news-and-blog/blog/2018/07/04/docker-io-debian-package-back-to-life/">Debian-provided package</a> in contrast to those provided by <a href="https://docs.docker.com/engine/install/ubuntu/">Docker Inc</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt install docker.io
</code></pre></div></div>

<p><strong>Issues:</strong> Running docker containers without <code class="language-plaintext highlighter-rouge">sudo</code> is a perennial issue in Ubuntu. Here’s some <a href="https://stackoverflow.com/questions/48957195/how-to-fix-docker-got-permission-denied-issue">context and solutions (link)</a>.</p>

<p>One fix I had to run was: <code class="language-plaintext highlighter-rouge">sudo chmod 666 /var/run/docker.sock</code></p>

<p>You can test your docker install by running:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run hello-world
</code></pre></div></div>

<h2 id="install-nvidia-docker">Install <code class="language-plaintext highlighter-rouge">nvidia-docker</code></h2>

<p>We need Nvidia’s <a href="https://github.com/NVIDIA/nvidia-docker">container toolkit (link)</a> to run GPU-accelerated docker containers. The install instructions are <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker">here</a>, but the short summary is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \
   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt update
sudo apt install -y nvidia-docker2
sudo systemctl restart docker
</code></pre></div></div>

<p>(This is the only weird extra package repository required for this setup.. phew.)</p>

<p>You can test <code class="language-plaintext highlighter-rouge">nvidia-docker</code> by running a CUDA-enabled container and running <code class="language-plaintext highlighter-rouge">nvidia-smi</code> within it, e.g.:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
</code></pre></div></div>

<h2 id="trying-out-a-tensorflow-container">Trying out a Tensorflow Container</h2>

<p>Ok–we’re ready to do some <strong>deep learning</strong> (really!)</p>

<p>Copying an example from <a href="https://www.tensorflow.org/install/docker">TensorFlow’s documentation</a>, you can test your install with:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \
   python -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
</code></pre></div></div>

<p>This should take quite a while to get started as it has to download the fairly large <code class="language-plaintext highlighter-rouge">tensorflow:latest-gpu</code> container, but that only has to be done once.</p>

<!-- ## A few example commands -->

<h2 id="setting-up-a-command-you-can-remember">Setting up a command you can remember</h2>

<p>All these arguments are going to be hard to remember. I’ve set up an aliased command in my <code class="language-plaintext highlighter-rouge">.bashrc</code> file to start up a Jupyter Notebook server that can see my <code class="language-plaintext highlighter-rouge">~/src</code> directory. This is the workflow I use for <em>most</em> of my ML research with my workstation.</p>

<p>Add this to <code class="language-plaintext highlighter-rouge">~/.bashrc</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>alias tfjupyter="docker run --gpus all -it -p 8888:8888 -v ~/src:/tf/notebooks tensorflow/tensorflow:latest-gpu-jupyter"
</code></pre></div></div>

<p>So now to start up a Jupyter Notebook with tensorflow and GPUs ready to go I just type <code class="language-plaintext highlighter-rouge">tfjupyter</code>.</p>

<h3 id="packages">Packages</h3>

<p>One downside here is that the docker container’s Python environment may not have every library that you want. For now, I’m planning to install extra packages inside my notebooks, e.g., something like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install keras-mdn-layer
</code></pre></div></div>

<h2 id="future-todos">Future TODOs</h2>

<ul>
  <li>get this working with Jupyter Lab.</li>
  <li>test out <code class="language-plaintext highlighter-rouge">docker.io</code> working without <code class="language-plaintext highlighter-rouge">sudo</code></li>
  <li>test out how this works with multiple users</li>
  <li>figure out a similar workflow for research using PyTorch (seems like its <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">similar to the above</a>)</li>
</ul>]]></content><author><name></name></author><category term="note" /><category term="machine-learning," /><category term="research" /><summary type="html"><![CDATA[For the last five years, I've seemed to have a more-or-less annual fight with my Linux workstations over installing CUDA to keep on doing GPU-accelerated]]></summary></entry><entry><title type="html">Building Synths in NoiseCraft</title><link href="https://charlesmartin.au/blog/2021/11/22/making-synths-noisecraft" rel="alternate" type="text/html" title="Building Synths in NoiseCraft" /><published>2021-11-22T00:00:00+00:00</published><updated>2021-11-22T00:00:00+00:00</updated><id>https://charlesmartin.au/blog/2021/11/22/making-synths-noisecraft</id><content type="html" xml:base="https://charlesmartin.au/blog/2021/11/22/making-synths-noisecraft"><![CDATA[<p>In this tutorial you’ll follow simple plans to create different electronic sounds and then use your knowledge to create a synthesiser. To create the synthesiser you’ll use <a href="https://noisecraft.app/">NoiseCraft</a>, a website that lets you build your own synthesiser by connecting together modules that create or modify electronic sounds.</p>

<p>The goals of the tutorial are:</p>

<ol>
  <li>Experiment with the fundamental parts of a synthesiser</li>
  <li>Experience how these parts connect to make different sounds</li>
  <li>Create a synthesiser and a short composition using it</li>
</ol>

<h2 id="task-0-opening-up-noisecraft">Task 0: Opening up NoiseCraft</h2>

<ul>
  <li>Get on to a laptop or desktop computer and open up a web browser.</li>
  <li>Type <code class="language-plaintext highlighter-rouge">https://noisecraft.app/</code> in the location bar and press <code class="language-plaintext highlighter-rouge">Enter</code></li>
</ul>

<p>You’ll see <strong>NoiseCraft</strong> website. It’s just a big empty space! That’s right, <strong>you</strong> have to create the synthesiser here, but that’s great because when <strong>you create something</strong> you understand it.</p>

<p>Let’s notice a few things:</p>

<ul>
  <li>The word “Play” is written in the top right corner. If you click “Play”, then your synthesiser will make sound (when you’ve built it), a “Stop” will appear and you can click that to stop the sound.</li>
</ul>

<p>It’s <strong>always</strong> important to know how to turn an electronic instrument <em>off</em> if you make a sound that’s too loud or that you don’t like.</p>

<ul>
  <li>Click the empty workspace. A “Create Node” menu will appear with different boxes and word.</li>
</ul>

<p>In NoiseCraft, a <strong>node</strong> is a building block of a synthesiser. You’ll create your synth by creating <strong>nodes</strong> and joining them together.</p>

<ul>
  <li>Click “AudiOut” in the “Create Node” menu, a little box with “AudioOut” on it will appear. Try dragging it around the workspace.</li>
</ul>

<p>The <strong>AudioOut</strong> node is connected to your computer’s speakers. Any sound you send to it will come out of your computer! (Once you click “Play”…)</p>

<p>Now we’re ready to make some sound!</p>

<h2 id="task-1-sound-generators-and-outputs">Task 1: Sound Generators and Outputs</h2>

<p>Synthesisers create complex sounds by combining and modifying simple <em>tone generators</em>. Let’s create some different tone generators and see what kind of sounds they make.</p>

<p><strong>Before starting</strong>: Turn the volume of your computer <strong>all the way down</strong>, if you’re using headphones take them out/off.</p>

<p><strong>Warning</strong>: These experiments can create <em>VERY LOUD SOUNDS</em>. Be <strong>very careful</strong> with sound when experimenting with synthesisers! Only put your earphones on when you know the volume is at a good level.</p>

<p>Create these nodes:</p>

<ul>
  <li>AudioOut (if you haven’t got one already)</li>
  <li>Sine</li>
  <li>Const</li>
</ul>

<p>Connect them together:</p>

<ul>
  <li>Connect the <code class="language-plaintext highlighter-rouge">out</code> of the Sine node to the <code class="language-plaintext highlighter-rouge">left</code> input of AudioOut.</li>
  <li>Connect the Const node to the <code class="language-plaintext highlighter-rouge">freq</code>  input of the Sine node.</li>
</ul>

<p>Most nodes have connection points on their left and right sides. The left-side points are <em>inputs</em> and the right-side ones are <em>outputs</em></p>

<p>So far, so good but where is the sound? First we have to set the sine node to play a sound we can hear.</p>

<ul>
  <li>Click the “0” in the const node.</li>
  <li>Type “440”</li>
  <li>press Enter</li>
</ul>

<p>Still no sound? Click “Play” and <strong>slowly</strong> turn your volume up a <strong>little bit</strong>. If you are using ear/headphones, put them near your ears before putting them all the way on to check that it’s not too loud.</p>

<p>Now you should hear a nice smooth <em>sine tone</em>! Ooooooooooooo.</p>

<blockquote>
  <p>Now try changing the frequency: type a different number into the const. What effect does this have on the sound?</p>
</blockquote>

<p>Let’s try some <em>other</em> tone generators. The sine tone is a pure, smooth, sound, but these other generators have different timbres:</p>

<ul>
  <li>Tri</li>
  <li>Saw</li>
  <li>Pulse</li>
  <li>Noise (this one doesn’t have a <code class="language-plaintext highlighter-rouge">freq</code> input!)</li>
</ul>

<blockquote>
  <p>Can you describe the sound of these tone generators?</p>
</blockquote>

<h2 id="task-2-changing-pitch-and-volume">Task 2: Changing Pitch and Volume</h2>

<p>So far we’ve just made a sound constantly plays at *maximum volume**. Let’s turn it down!</p>

<p>Create these nodes:</p>

<ul>
  <li>Mul</li>
  <li>Knob</li>
</ul>

<p>Disconnect any tone generators from the AudioOut then connect</p>

<ul>
  <li>a tone generator node to the <code class="language-plaintext highlighter-rouge">in0</code> of the Mul node</li>
  <li>the Knob to the <code class="language-plaintext highlighter-rouge">in1</code> of the Mul node</li>
  <li>the <code class="language-plaintext highlighter-rouge">out</code> of the AudioOut to one (or both) of the inputs of the AudioOut node.</li>
</ul>

<p>Press play and adjust the Knob to turn up the volume!</p>

<blockquote>
  <p>“Mul” stands for “multiply”, but why do we multiply things together to change the volume? (think about what happens when you multiply a number by zero…)</p>
</blockquote>

<p>Now let’s change pitch:</p>

<ul>
  <li>Disconnect the Const from your tone generator.</li>
  <li>Create another Knob</li>
  <li>Connect the output of the Knob to the <code class="language-plaintext highlighter-rouge">freq</code> of the tone generator.</li>
  <li>Double click the knob (a little menu appears) and change <code class="language-plaintext highlighter-rouge">maxVal</code> to 1000.</li>
</ul>

<p>Now you can turn the knob to select a frequency or pitch.</p>

<blockquote>
  <p>Can you work out the frequency of particular notes on your instrument or a piano?</p>
</blockquote>

<h2 id="task-3---playing-notes">Task 3 - Playing Notes</h2>

<p>Create <strong>MidiIn</strong> node: this lets you play your synthesiser with the keyboard of your computer.</p>

<ul>
  <li>Connect the <code class="language-plaintext highlighter-rouge">freq</code> output from the MidiIn node to the freq input of your tone generator (instead of the frequency Knob)</li>
  <li>press the letters “a” to “l” on your keyboard to play some notes!</li>
</ul>

<h2 id="task-4---see-the-sound">Task 4 - SEE the sound</h2>

<p>Create a <strong>Scope</strong> node. This visualises the signal of any output.</p>

<ul>
  <li>connect the <code class="language-plaintext highlighter-rouge">out</code> of a tone generator to the input of the Scope.</li>
  <li>press some letters and you’ll see the sound waves visualised in the Scope</li>
</ul>

<p>The Scope is probably showing a very messy visualisation right now. Try connecting a <em>very low frequency</em> tone to the Scope and you should be able to see more detail (e.g., 5Hz). If you try to visualise a very low frequency can you see why the tones have their names?</p>

<h2 id="task-5---shaping-notes">Task 5 - Shaping Notes.</h2>

<p>So far our notes have a kind of boring <em>shape</em>. That is, they are either <strong>on</strong> (playing) or <strong>off</strong>. Think about the <em>shape</em> of sounds produced by other instruments, they might start quietly, get louder, and fade away over time.</p>

<p>The <em>shape</em> of a note is called the <em>envelope</em>. It’s the chunk of time that a note lives in with changes in volume and timbre of a sound over that time.</p>

<p><img src="https://cs.anu.edu.au/courses/comp2300/assets/lectures/synth/envelope-sound.png" alt="We can change the amplitude over the [envelope](https://cs.anu.edu.au/courses/comp2300/lectures/digital-synthesis/#/amplitude-envelope) to give a note a sonic “shape”." /></p>

<p>So now we are going to create an envelope generator (EG) for our synth in NoiseCraft.</p>

<p>NoiseCraft has an EG node called <strong>ADSR</strong>. This creates envelope shapes in a particular 4-stage envelope which is popular for synthesisers:</p>

<p><img src="https://cs.anu.edu.au/courses/comp2300/assets/lectures/synth/adsr.png" alt="[ADSR](https://cs.anu.edu.au/courses/comp2300/lectures/digital-synthesis/#/adsr-envelope) stands for &quot;attack&quot;, &quot;decay&quot;, &quot;sustain&quot;, &quot;release&quot;. These are the four phases of a pitched note." /></p>

<p>Create an <strong>ADSR</strong> node. An EG also creates signals but instead of listening to it, we use it to control other nodes.</p>

<ul>
  <li>Hook up the output of the ADSR node to the volume control <strong>Mul</strong> node of your synth.</li>
  <li>Hook up the <code class="language-plaintext highlighter-rouge">gate</code> input of the ADSR node to the <code class="language-plaintext highlighter-rouge">gate</code> output of your MidiIn** node</li>
  <li>Create four Knobs and hook them up to the <code class="language-plaintext highlighter-rouge">att</code>, <code class="language-plaintext highlighter-rouge">dec</code>, <code class="language-plaintext highlighter-rouge">sus</code>, and <code class="language-plaintext highlighter-rouge">real</code> inputs of the ADSR node.</li>
</ul>

<p>Adjust the knobs: try setting attack to 0.2, decay to 0.4, sustain to 0.5 and release to 0.8.</p>

<p>Now if you play your synth with the keyboard you’ll notice that it take a little bit of time for the notes to fade in when you press down a key, and to fade away when you lift your finger from the keyboard.</p>

<p>What kind of envelopes can you create with this setup? Can you mimic an acoustic instrument that you play?</p>

<h2 id="task-6---additive-synthesis">Task 6 - Additive Synthesis</h2>

<p>So far we have only used a single simple tone generator to create sounds. These sound cool, but we haven’t had any control over <em>timbre</em>, or sound colour, of the synthesiser except to change the basic waveform.</p>

<p>Think: What does <em>timbre</em> mean to you? How do you control timbre on your instrument?</p>

<p>In general, synthesisers use more complex techniques to control <em>timbre</em> and create all kinds of interesting sounds. One simple way to do this is to take more than one tone generator and <em>add the sounds together</em>. This is called “additive synthesis”.</p>

<ul>
  <li>make another tone generator node (of the same or different type as one you already are using)</li>
  <li>create an <strong>Add</strong> node.</li>
  <li>hook up the outputs of both of your tone generators to the inputs of the <strong>Add</strong> node</li>
  <li>hook up the output of the <strong>Add</strong> node to the volume control/EG in your synth.</li>
</ul>

<p>Now we need to control the frequency of our second tone generator.</p>

<ul>
  <li>connect the <code class="language-plaintext highlighter-rouge">freq</code> output of the MidiIn node to the <code class="language-plaintext highlighter-rouge">freq</code> in of your second tone generator.</li>
  <li>Play some notes!</li>
</ul>

<p>Now, this may sound a <em>bit</em> different, but not very much so. This is because both tone generators are set to exactly the same frequency. Let’s change that.</p>

<ul>
  <li>Create a <strong>Mul</strong> node and a <strong>Const</strong> node.</li>
  <li>Use these to multiply the <code class="language-plaintext highlighter-rouge">freq</code> output of the MidiIn node before sending it to your second tone generator.</li>
</ul>

<p>Try some different numbers for multiplying the tone generators. Try some integers (e.g., 2, 3, 4) and some numbers with a decimal point (e.g., 1.4).</p>

<p>What do these different additive sounds sound like?</p>

<p>Can you create a perfect fifth between your tone generators?</p>

<p>One cool sound is to set the second tone generator very slightly out of tune with the first, e.g., set the Const node to <code class="language-plaintext highlighter-rouge">1.01</code>. Try it!</p>

<p>If you create a couple of tone generators, you can still add them together, but you might have to introduce some volume controls to mix their volumes together. Usually we would set the <em>lower</em> tones to have a higher volume and the <em>higher</em> tones are a bit quieter.</p>

<h2 id="task-7---comparing-to-a-famous-synthesiser">Task 7 - Comparing to a famous synthesiser.</h2>

<p>So what have we learned so far? Are we making a synth yet?</p>

<p>Let’s compare our knowledge to the <a href="https://www.moogmusic.com/products/minimoog-model-d">Moog Minimoog Model D</a>, a very famous synth from the 1970s.</p>

<p>In fact, the Minimoog was so successful that it’s design influenced many other synthesisers and there are recreations of the Minimoog in software and hardware (there’s a <a href="https://www.bettermusic.com.au/behringer-model-d">Behringer Model-D</a> over at Better Music if you want to have a look).</p>

<p>Find a picture of the Minimoog control panel and take a second to look carefully. There are a <em>lot</em> of knobs, but your synth probably has a lot at this point to so that’s nothing to be afraid of.</p>

<p>The Minimoog control panel is divided into sections from left to right:</p>

<ul>
  <li>
    <p>At the far left, there’s a Controllers section which adjusts the input from a keyboard, similarly to your MidiIn node.</p>
  </li>
  <li>
    <p>The next section is the Oscillator Bank. An “oscillator” is just another name for a “tone generator”.</p>
  </li>
  <li>
    <p>The Minimoog has three oscillators. You might be able to see a knob for setting the waveform shape of each one and a knob for setting the octave. Some synthesisers measure octaves in “feet” (as in the unit of length)—this is related to the measurements of organ pipes (believe it or not!) which get lower as they get longer. There are <em>two</em> knobs for detuning the second and third oscillators against the first.</p>
  </li>
  <li>
    <p>Next we have a “mixer” section–just like your additive synth. There’s also a mixer for “noise” and the signal from an external input.</p>
  </li>
  <li>
    <p>The next section is complicated with two parts stacked up. Looking at the bottom one first, this is called “Loudness Contour”, which we call “Envelope Generator”. It has three familiar labels: attack, decay, and sustain.</p>
  </li>
  <li>
    <p>The section above that is a <em>filter</em> section, which has it’s <em>own</em> separate envelope. This is incredibly important, as it gives us a way to shape timbre over time, but we’ll get to filters in the next task.</p>
  </li>
  <li>
    <p>The last section just has a power switch and a main volume knob.</p>
  </li>
</ul>

<p>So you now should have an idea of what MOST of the knobs on the Minimoog might do. Not so complicated after all huh?</p>

<h2 id="task-8---filter">Task 8 - Filter</h2>

<p>Have you noticed that some of the sounds from the tone generators are quite harsh? This is particularly the case for the <em>pulse</em> and <em>sawtooth</em> tones. These tone generators don’t sounds like they are going to be very useful. Luckily, we have a sound design tool that can help control a harsh timbre and give us extra possible sound colours: a filter! Let’s add one to our synth.</p>

<ul>
  <li>make a <strong>Filter</strong> node and place it in between the tone generators and ADSR section in your synth signal chain.</li>
  <li>Make two knobs and connect one to the <code class="language-plaintext highlighter-rouge">cutoff</code> input and one to the <code class="language-plaintext highlighter-rouge">reso</code> input.</li>
  <li>Turn the <code class="language-plaintext highlighter-rouge">cutoff</code> knob to 1 and the <code class="language-plaintext highlighter-rouge">reso</code> knob to 0.</li>
</ul>

<p>(NB: In the above, the <em>exact</em> connections to make are left out, you should connect the <code class="language-plaintext highlighter-rouge">in</code> and <code class="language-plaintext highlighter-rouge">out</code> connectors so that the synth sound goes <em>through</em> the filter).</p>

<p>Play some sounds on your synth and (simultaneously) try turning the <code class="language-plaintext highlighter-rouge">cutoff</code> knob down to 0. What difference does it make?</p>

<p>Turn the <code class="language-plaintext highlighter-rouge">reso</code> knob up to 0.8 and try the same thing. What happens now?</p>

<p>The <em>filter</em> in NoiseCraft is actually a low-pass filter (LPF), it filters out higher frequency sound and lets lower frequency sound pass through[^A low-pass filter is one of the most used types of filters in electronic music. If someone says a synth has a “filter”, it’s probably a resonant LPF just like this one.]. In practice, it lets the <em>fundamental</em> frequency through and removes some of the <em>overtones</em>.</p>

<p>The <code class="language-plaintext highlighter-rouge">cutoff</code> knob controls the frequency where the filter “cuts off” higher sounds. The <code class="language-plaintext highlighter-rouge">reso</code> (resonance) knob emphasises the sound at the <code class="language-plaintext highlighter-rouge">cutoff</code> frequency. If you turn the resonance up quite high, you start to hear a kind of “ringing” as you move the cutoff frequency up and down.</p>

<blockquote>
  <p>A low-pass filter is lot like the sound you get while singing and putting your hand over your mouth. Try it and see if you can get a similar effect to “cutoff” and “reso” by shaping you hand and mouth in different ways.</p>
</blockquote>

<p>Well now you can tame those harsh synth sounds with a filter. To take your filtering a step further, we can change the <code class="language-plaintext highlighter-rouge">cutoff</code> frequency over time using an envelope generator.</p>

<ul>
  <li>Make a new <strong>ADSR</strong> node, just like the one you made for volume and connect it to the <code class="language-plaintext highlighter-rouge">cutoff</code> input of your <strong>Filter</strong>.</li>
  <li>Connect the filter ADSR generator’s <code class="language-plaintext highlighter-rouge">gate</code> input to the same <code class="language-plaintext highlighter-rouge">gate</code> output of your <strong>MidiIn</strong> or <strong>MonoSeq</strong> node that you are using for controlling pitch and rhythm.</li>
</ul>

<p>Experiment with different envelope shapes for your filter. What sounds good to you?</p>

<p>One last thing: A low-pass filter <em>removes</em> sound, so we often call synthesisers that use filters as their main way to shape sound <em>subtractive</em> synths.</p>

<h2 id="task-9---sequencer">Task 9 - Sequencer</h2>

<p>Playing notes with the computer or MIDI keyboard with <strong>MidiIn</strong> in fun, but it’s a slow way to compose a song. Many synthesisers are used with a <em>sequencer</em> [^In fact, most synthesisers have some kind of sequencer built-in.]that can play back phrases of notes in a loop (repeated over and over) this lets musicians compose parts to songs, or create loop-based electronic  music.</p>

<p>NoiseCraft has a sequencer called <strong>MonoSeq</strong> that you can use to compose phrases. Let’s try it out.</p>

<ul>
  <li>Create a <strong>MonoSeq</strong> node and a <strong>Clock</strong> node.</li>
  <li>Connect the <code class="language-plaintext highlighter-rouge">freq</code> out of the <strong>MonoSeq</strong> to <code class="language-plaintext highlighter-rouge">freq</code> input of your tone generator(s) and the <code class="language-plaintext highlighter-rouge">gate</code> output to your envelope generator(s).</li>
  <li>Connect the <strong>Clock</strong>’s output to the <code class="language-plaintext highlighter-rouge">clock</code> input on the <strong>MonoSeq</strong>.</li>
</ul>

<p>Now that everything is hooked up, press play and you’ll hear… nothing! You need to enter in a sequence first. The <strong>MonoSeq</strong> node has a nice red grid that represents a musical phrase. Each column is a 16th note and each row represents a different pitch. Enter some notes and create a great melody!</p>

<p>Here’s a few things to try:</p>

<ul>
  <li>change tempo by adjusting your <strong>Clock</strong> node.</li>
  <li>change the scale or root pitch of your <strong>MonoSeq</strong> object</li>
  <li>experiment to figure out what the buttons on the <strong>MonoSeq</strong> do</li>
</ul>

<h2 id="tast-10-compose-some-synth-music">Tast 10: Compose some synth music</h2>

<p>Now you have all the knowledge you need to create a short synthesiser composition. For this task you have to:</p>

<ul>
  <li>create a synthesiser using knowledge from the earlier tasks. Your synthesiser should have at least two tone generators, a filter, and an envelope, but can have more.</li>
  <li>create at least one sequencer with a sequence containing your composition.</li>
  <li>include some “live” elements to your composition by planning to turn some knobs, or change your sequence(s) during your performance.</li>
  <li>perform your composition for the class.</li>
</ul>

<p>When composing with synthesisers keep in mind that the sequence is only <em>one part</em> of your composition. The timbre of your synthesiser can also be composed (we call this “sound design”) and can be changed over time during your composition. Genres like techno are <em>mainly</em> about changes in <em>timbre</em> and <em>texture</em> rather than melody.</p>

<h2 id="finding-out-more">Finding out more</h2>

<p>Well now you know a <em>lot</em> about how a typical synthesiser works, and further, you know how to create one[^Pretty rad huh? How many other musical instruments do you know how to build?].</p>

<p>Having created all the synths in this tutorial, you might like to use the “Browse” feature in NoiseCraft to look at synthesisers that other people have created. Some of these are quite complicated, but they use all the same building blocks that we have discussed so you will be able to look at how the nodes are connected together and see how they work!</p>

<h2 id="extra-glossary">Extra: Glossary</h2>

<p>There are many technical terms in used in electronic music and sometimes multiple words for the same thing (sometimes different manufacturers use different terms for historical reasons). Here’s a list of typical translations from the terminology used above to other terms that you might see.</p>

<ul>
  <li>Tone Generator: Oscillator, Voltage Controlled Oscillator, VCO</li>
  <li>ADSR: Envelope Generator, EG, Slope Generator, Attack/Decay</li>
  <li>LFO: Low Frequency Oscillator</li>
  <li>LPF: Low-pass filter</li>
  <li>BPF: Band-pass filter</li>
  <li>HPF: High-pass filter</li>
</ul>

<p>Music <em>does</em> tend to have a lot of overlapping technical terms. Can you think of any other musical concepts that have multiple names? (Think about how you describe <em>fast</em> in music, and the name for a note that goes for one beat of a bar).</p>]]></content><author><name>Charles Martin</name></author><category term="workshop" /><summary type="html"><![CDATA[In this tutorial you'll follow simple plans to create different electronic sounds and then use your knowledge to create a synthesiser. To create the]]></summary></entry></feed>