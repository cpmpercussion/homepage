I"ú<p>Since about 2011, Iâ€™ve been performing music with various kinds of touch-screen devices in percussion ensembles, new music groups, improvisation workshops, installations, as well as my dedicated iPad group, 
<a href="https://charlesmartin.com.au/metatone/">Ensemble Metatone</a>. Most of these events were recorded; detailed touch and gestural information was collected including classifications of each ensemble memberâ€™s gesture every second during each performance. Since moving to Oslo, however, I donâ€™t have an iPad band! This leads to the question: Given all this performance data, can I make an artificial touch-screen ensemble using deep neural networks?</p>

<p><img src="/assets/squarespaceblog/2017-05-02-neuralensembleidea.jpg" alt="" /></p>
:ET