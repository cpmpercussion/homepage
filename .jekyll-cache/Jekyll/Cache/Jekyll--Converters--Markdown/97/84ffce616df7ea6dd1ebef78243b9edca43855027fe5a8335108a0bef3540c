I"ü.<p>IMPS is a system for predicting musical control data in live performance. It uses a mixture density recurrent neural network (MDRNN) to observe control inputs over multiple time steps, predicting the next value of each step, and the time that expects the next value to occur. It provides an input and output interface over OSC and can work with musical interfaces with any number of real valued inputs (we‚Äôve tried from 1-8). Several interactive paradigms are supported for call-response improvisation, as well as independent operation, and ‚Äúfiltering‚Äù of the performer‚Äôs input. Whenever you use IMPS, your input data is logged to build up a training corpus and a script is provided to train new versions of your model.</p>

<!-- ![MIT License](https://img.shields.io/github/license/cpmpercussion/keras-mdn-layer.svg?style=flat) -->
<!-- [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2580176.svg)](https://doi.org/10.5281/zenodo.2580176) -->

<p><img src="https://github.com/cpmpercussion/imps/raw/master/images/predictive_interaction.png" alt="Predictive Musical Interaction" /></p>

<p>Here‚Äôs a <a href="https://www.youtube.com/embed/Kdmhrp2dfHw">demonstration video showing how IMPS can be used with
different musical
interfaces.</a></p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Kdmhrp2dfHw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="installation">Installation</h2>

<p>IMPS is written in Python with Keras and TensorFlow Probability, so it should work on any platform where Tensorflow can be installed. Python 3 is required.</p>

<p>First you should clone this repository or download it to your computer:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/cpmpercussion/imps.git
cd imps
</code></pre></div></div>

<p>The python requirements can be installed as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -r requirements.txt
</code></pre></div></div>

<p>Some people like to keep Python packages separate in virtual environments, if that‚Äôs you, here‚Äôs some terminal commands to install:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virtualenv --system-site-packages -p python3 venv
source venv/bin/activate
pip install -r requirements.txt
</code></pre></div></div>

<h2 id="how-to-use">How to use</h2>

<p>There are four steps for using IMPS. First, you‚Äôll need to setup your musical interface to send it OSC data and receive predictions the same way. Then you can log data, train the MDRNN, and make predictions using our provided scripts.</p>

<h3 id="1-connect-music-interface-and-synthesis-software">1. Connect music interface and synthesis software</h3>

<p>You‚Äôll need:</p>

<ul>
  <li>A music interface that can output data as OSC.</li>
  <li>Some synthesiser software that can take OSC as input.</li>
</ul>

<p>These could be the same piece of software or hardware!</p>

<p>You need to decide on the number of inputs (or dimension) for your predictive model. This is the number of continuous outputs from your interface plus one (for time). So for an interface with 8 faders, the dimension will be 9.</p>

<p>Now you need your music interface to send messages to IMPS over OSC. The default address for IMPS is: localhost:5001. The messages to IMPS should have the OSC address <code class="language-plaintext highlighter-rouge">/interface</code>, and then a float between 0 and 1 for each continuous output on your interface, e.g.:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/interface 0 0.5 0.23 0.87 0.9 0.7 0.45 0.654
</code></pre></div></div>

<p>For an 8-dimensional interface.</p>

<p>Your synthesiser software or interface needs to listen for messages from the IMPS system as well. These have the same format with the OSC address <code class="language-plaintext highlighter-rouge">/prediction</code>. You can interpret these as interactions predicted to occur right when the message is sent.</p>

<p>Here‚Äôs an example diagram for our 8-controller example, the <a href="https://www.musictribe.com/Categories/Behringer/Computer-Audio/Desktop-Controllers/X-TOUCH-MINI/p/P0B3M">xtouch mini controller</a>.</p>

<p><img src="https://github.com/cpmpercussion/imps/raw/master/images/IMPS_connection_example.png" alt="Predictive Musical Interaction" /></p>

<p>In this example we‚Äôve used Pd to connect the xtouch mini to IMPS and to synthesis sounds. Our Pd mapping patch takes data from the xtouch and sends <code class="language-plaintext highlighter-rouge">/interface</code> OSC messages to IMPS, it also receives <code class="language-plaintext highlighter-rouge">/prediction</code> OSC message back from IMPS whenever they occur. Of course, whenever the user performs with the controller, the mapping patch sends commands to the synthesiser patch to make sound. Whenever <code class="language-plaintext highlighter-rouge">/prediction</code> messages are received, these also trigger changes in the synth patch, and we also send MIDI messages back to the xtouch controller to update its lights so that the performer knows what IMPS is predicting.</p>

<p>So what happens if IMPS and the performer play at the same time? In this example, it doesn‚Äôt make sense for both to control the synthesiser at the same time, so we set IMPS to run in ‚Äúcall and response‚Äù mode, so that it only makes predictions when the human has stopped performing. We could also set up our mapping patch to use prediction messages for a different synth and use one of the simultaneous performance modes of IMPS.</p>

<h3 id="2-log-some-training-data">2. Log some training data</h3>

<p>You use the <code class="language-plaintext highlighter-rouge">predictive_music_model</code> command to log training data. If your interface has N inputs the dimension is N+1:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python predictive_music_model.py --dimension=(N+1) --log
</code></pre></div></div>

<p>This command creates files in the <code class="language-plaintext highlighter-rouge">logs</code> directory with data like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-01-17T12:37:38.109979,interface,0.3359375,0.296875,0.5078125
2019-01-17T12:37:38.137938,interface,0.359375,0.296875,0.53125
2019-01-17T12:37:38.160842,interface,0.375,0.3046875,0.1953125
</code></pre></div></div>

<p>These CSV files have the format: timestamp, source of message (interface or rnn), x_1, x_2, ‚Ä¶,  x_N.</p>

<p>You can log training data without using the RNN with the <code class="language-plaintext highlighter-rouge">o</code> switch (user only) if you like, or use a partially trained RNN and then collect more data.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python predictive_music_model.py --dimension=(N+1) --log -o
</code></pre></div></div>

<p>Every time you run the <code class="language-plaintext highlighter-rouge">predictive_music_model</code>, a new log file is created so that you can build up a significant dataset!</p>

<h3 id="3-train-an-mdrnn">3. Train an MDRNN</h3>

<p>There‚Äôs two steps for training: Generate a dataset file, and train the predictive model.</p>

<p>Use the <code class="language-plaintext highlighter-rouge">generate_dataset</code> command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python generate_dataset --dimension=(N+1)
</code></pre></div></div>

<p>This command collates all logs of dimension N+1 from the logs directory and saves the data in a compressed <code class="language-plaintext highlighter-rouge">.npz</code> file in the datasets directory. It will also print out some information about your dataset, in particular the total number of individual interactions. To have a useful dataset, it‚Äôs good to start with more than 10,000 individual interactions but YMMV.</p>

<p>To train the model, use the <code class="language-plaintext highlighter-rouge">train_predictive_music_model</code> command‚Äîthis can take a while on a normal computer, so be prepared to let your computer sit and think for a few hours! You‚Äôll have to decide what <em>size</em> model to try to train: <code class="language-plaintext highlighter-rouge">xs</code>, <code class="language-plaintext highlighter-rouge">s</code>, <code class="language-plaintext highlighter-rouge">m</code>, <code class="language-plaintext highlighter-rouge">l</code>, <code class="language-plaintext highlighter-rouge">xl</code>. The size refers to the number of LSTM units in each layer of your model and roughly corresponds to ‚Äúlearning capacity‚Äù at a cost of slower training and predictions.
It‚Äôs a good idea to start with an <code class="language-plaintext highlighter-rouge">xs</code> or <code class="language-plaintext highlighter-rouge">s</code> model, and the larger models are more relevant for quite large datasets (e.g., &gt;1M individual interactions).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train_predictive_music_model.py --dimension=(N+1) --modelsize=xs --earlystopping
</code></pre></div></div>

<p>It‚Äôs a good idea to use the ‚Äúearlystopping‚Äù parameter to stop training after the model stops improving for 10 epochs.</p>

<h3 id="4-perform-with-your-predictive-model">4. Perform with your predictive model</h3>

<p>Now that you have a trained model, you can run this command to start making predictions:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python predictive_music_model.py -d=(N+1) --modelsize=xs --log -c
</code></pre></div></div>

<p>THe <code class="language-plaintext highlighter-rouge">--log</code> switch logs all of your interactions as well as predictions for later re-training. (The dataset generator filters out RNN records so that you only train on human sourced data).</p>

<p>PS: the three scripts respond to the <code class="language-plaintext highlighter-rouge">--help</code> switch to show command line options. If there‚Äôs something not documented or working, it would be great if you add an issue above to let me know or get in touch on Twitter at <a href="https://twitter.com/cpmpercussion">@cpmpercussion</a>.</p>

<h2 id="more-about-mixture-density-recurrent-neural-networks">More about Mixture Density Recurrent Neural Networks</h2>

<p>IMPS uses a mixture density recurrent neural network MDRNN to make predictions. This machine learning architecture is set up to predict the next in a sequence of multi-valued elements. The recurrent neural network uses LSTM units to remember information about past inputs and use this to help make decisions. The mixture density model at the end of the network allows continuous multi-valued elements to be sampled from a rich probability distribution.</p>

<p>The network is illustrated here‚Äîevery time IMPS receives an interaction message from your interface, it is sent to thorugh the LSTM layers to produce the parameters of a Gaussian mixture model. The predicted next interaction is sampled from this probability model.</p>

<p><img src="https://github.com/cpmpercussion/imps/raw/master/images/mdn_diagram.png" alt="A Musical MDRNN" /></p>

<p>The MDRNN is written in Keras and uses the <a href="https://github.com/cpmpercussion/keras-mdn-layer">keras-mdn-layer</a> package. There‚Äôs more info and tutorials about MDNs on <a href="https://github.com/cpmpercussion/keras-mdn-layer">that github repo</a>.</p>
:ET