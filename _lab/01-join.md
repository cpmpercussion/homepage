---
layout: page
title: Join the SMCClab
permalink: /students/
summary: How to learn about sound, music and creative computing in the lab.
---

![Teaching header, a metatone performance with friends]({{site.baseurl}}/assets/images/performing/metatone-header.jpg)

**Currently Recruiting!** I'm currently looking for **PhD students** in the field of musical machine learning, particularly for projects focussed on using AI/ML in live performance!

If you're passionate about _music, interaction, and computing_, and want to be a part of [an innovative school](https://comp.anu.edu.au) at one of the [world's top universities](https://anu.edu.au), get in touch through [ANU](https://comp.anu.edu.au/people/charles-martin/)!

There are three ways to learn about sound, music and creative computing in my lab:

1. Take one of my classes (see below)---I teach Bachelor- and Master-level classes for computing students and other students around ANU.

2. Apply to do an [Honours or Master project](https://comp.anu.edu.au/study/projects/) with me. If you apply to do a project but haven't completed any creative computing courses at ANU, I will probably advise you to go back to step 1 and take a course first to learn the basics.

3. Apply to become a [PhD or MPhil student](https://comp.anu.edu.au/study/research/) in my lab. This is for folks who already have excellent results in a computing or creative technology degree and experience in creative computing. You'll need to get in touch with me well before applications are due.

{:.info-box}
I will next recruit student projects in January 2024. I will next recruit PhD students in the application round due in April 2024.

## Courses

I currently teach:

- [Art and Interaction in New Media (COMP1720/COMP6720)](https://cs.anu.edu.au/courses/comp1720) 
- [Sound and Music Computing (COMP4350/8350)](https://cs.anu.edu.au/courses/laptop-ensemble) 

{% comment %}
I have previously taught:

- [Computer Organisation and Program Execution (COMP2300/COMP6300)](https://cs.anu.edu.au/courses/comp2300) at ANU (2019, 2020, 2022).
- [Advanced Topics in AI for Intelligent Systems (IN5490)](https://www.uio.no/studier/emner/matnat/ifi/IN5490) at the University of Oslo,
- [Drums and Percussion at the ANU School of Music](https://music.cass.anu.edu.au/performance-ensemble/drums-and-percussion).
{% endcomment %}


![teaching a group]({{site.baseurl}}/assets/images/teaching/bela-workshop-header.jpg)

## PhD/MPhil Research Students

I supervise PhD, Master, and Honours students at the ANU and at University of Oslo. My previous students' published work is [listed here]({% link _lab/02-alumni.md %}). You can see my [current students on my lab page]({% link _lab/index.md %}).

My students have gone on to work at Google, Seeing Machines, Oslo Metropolitan University, and other wonderful places.

I'm only able to accept students who have excellent results in a computing or creative technology degree at the Master or Honours level and who have existing experience in computing for a creative field. Interest in music or art is not enough, experience needs to be clearly demonstrated through projects and (hopefully) publications or a thesis.

Prospective students should also have a look at my list of [focus areas]({% link _posts/2023-08-26-projects-in-24.md %})}.

## Student Projects

I supervise student projects in sound and music computing, creativity support systems, computational creativity, music technology, and interactive systems. I'm interested in creating and studying new computing systems at the nexus of creative arts, physical embodiment, interaction, and intelligence.

My available projects are listed on the [ANU School of Computing student project page](https://comp.anu.edu.au/study/projects/).

Projects should align with my current [focus areas]({% link _posts/2023-08-26-projects-in-24.md %})}:

- spatial interaction (creating new musical instruments in VR/AR or with movement sensors)
- generative creative gestures with AI/ML
- guiding collaborative performances with AI models
- creating intelligent instruments in hardware

![Charles Martin Background Areas]({{site.baseurl}}/assets/images/charlesmartin-background.jpg)

Experience with a creative field is a must. Computing students should have taken either [COMP1720/6720 Art and Interaction Computing]() or [COMP4350/8350 Sound and Music Computing]() and obtained excellent results.

{% comment %}
### Creating a New Interface for Musical Expression

Computers and electronics give us so many new opportunities to create new kinds of musical instruments (and new kinds of music). We often think of these systems as containing an _interface_ between a human user and sound synthesisers. The interface can either be a piece of computer software (e.g., a web app, or a p5.js program), or hardware (e.g., an Arduino with custom sensors). [New Interfaces for Musical Expression](https://nime.org) is a research field where we explore how these new software and hardware-based instruments work and what kinds of music they can make.

I've created _lots_ of different kinds of new musical interfaces, but in this project, it's a good opportunity to create something that you invent or to take a new idea further than it has been before.

### Requirements:

- strong interest in music and interaction

### Nice to have: 

- experience in hardware (e.g., Arduino) 
- experience in interactive media software (e.g., [p5.js](p5js.org))
- experience in computer music (e.g., [ANU Laptop Ensemble](https://cs.anu.edu.au/courses/comp2710-lens/)) or creative arts (e.g., [Art and Interaction Computing](https://cs.anu.edu.au/courses/comp1720/)).



### Discriminating real from neural generated music

Most generative music systems just create new music, this project would involve training discriminator neural networks to tell if music was created by a human, or a cutting edge ANN such as [Music Transformer](https://magenta.tensorflow.org/music-transformer) or [PerformanceRNN](https://magenta.tensorflow.org/performance-rnn).

### Generating Harmony from Melody

This project involves creating a sequence-to-sequence ANN that can generate harmony, or counter-melodic parts for a given melody. The focus here would be to deploy this network in a music creating app such as [MicroJam](https://microjam.info).

### Generating "improved" versions of a melody/MIDI content

The idea of this project is to take an existing melody created in an interactive music system and change its style, or improve it in some way. It could involve modifying the melody to fit a particular harmony, or create a more appealing melodic shape. Various sequence-to-sequence ANN techniques could be applied here, and such a system could have applications in musical performance and education.

### Neural Networks for Generating Digital Audio

The idea of generating digital audio directly from a neural network is exciting and there are stunning results from examples such as [Wavenet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/), [SampleRNN](https://arxiv.org/abs/1612.07837), [FFTNet](https://gfx.cs.princeton.edu/pubs/Jin_2018_FAR/), and [WaveGan](https://github.com/chrisdonahue/wavegan). Projects in this area will involve finding new application areas for neural audio generation and developing focussed and fast algorithms with appropriate training data for use in these areas. In particular, the use of these networks for creative arts is still being actively explored.

### Evolutionary Music Making

[Making music with evolutionary algorithms](https://en.wikipedia.org/wiki/Evolutionary_music) has a long history, but is yet to break into mainstream music technology systems. In this project, you will develop an interactive music application (e.g., web or mobile app) for generating and assessing music created using an evolutionary algorithm. A novel approach might involve the human user evaluating some generated sounds, while others are analysed by an automatic system using MIR techniques or a discriminator neural network.

### New Applications of the Mixture Density Recurrent Neural Network

The MDRNN is an exciting sequence model that can generate multiple continuous valued signals from a Gaussian mixture model at each step in time. This project will involve applying and extending my [Keras MDRNN models](https://github.com/cpmpercussion/keras-mdn-layer) into new applications in the creative arts and beyond. We've tried using the MDRNN for voice synthesis, motion capture data synthesis, and musical control data synthesis, but there are lots of other potential applications waiting for you to discover, e.g.: predicting future sensor values, generating robot movements, generating world models for video games or real life situations etc.
{% endcomment %}
